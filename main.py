# region å¯¼å…¥
import asyncio
import hashlib
import json
import re
import time
from pathlib import Path
from urllib.parse import urlparse

import httpx
from astrbot.api import AstrBotConfig, logger
from astrbot.api.event import AstrMessageEvent, filter
import astrbot.api.message_components as Comp
from astrbot.api.star import Context, Star, register

from .core.bilibili import BILI_MESSAGE_PATTERN, BilibiliMixin
from .core.common import SizeLimitExceeded, get_bili_cookies_file
from .core.douyin import DOUYIN_MESSAGE_PATTERN, DouyinExtractor
from .core.douyin.handler import DouyinMixin
from .core.xiaohongshu import (
    XHS_MESSAGE_PATTERN,
    XiaohongshuCardRenderer,
    XiaohongshuExtractor,
    find_default_font,
)
from .core.xiaohongshu.handler import XiaohongshuMixin
# endregion

# region è¿è¡Œæ—¶å¸¸é‡
TASK_NAME_PREFIX = "myparser-parse"
# endregion

# region MyParser ç±»
@register("astrbot_plugin_link_resolver", "acacia", "è§£æ & ä¸‹è½½ Bilibili/æŠ–éŸ³/å°çº¢ä¹¦", "1.0.8")
class MyParser(BilibiliMixin, DouyinMixin, XiaohongshuMixin, Star):
    def __init__(self, context: Context, config: AstrBotConfig | dict | None = None):
        super().__init__(context)
        self.context = context
        self.config = config or context.get_config()
        self._migrate_legacy_config()
        # æ³¨æ„ï¼šå¿…é¡»åœ¨ _active_parse_tasks åˆå§‹åŒ–ä¹‹å‰è°ƒç”¨ï¼›
        # è¯¥æ–¹æ³•é€šè¿‡ asyncio.all_tasks() æ‰«ææ¸…ç†æ—§ä»»åŠ¡ï¼Œä¸ä¾èµ–å®ä¾‹ä»»åŠ¡æ± ã€‚
        self._cancel_previous_parse_tasks()
        self._active_parse_tasks: set[asyncio.Task] = set()
        self.douyin_extractor = DouyinExtractor()
        self.xhs_extractor = XiaohongshuExtractor()
        self.xhs_renderer = XiaohongshuCardRenderer(find_default_font())
        self._refresh_config()

    # region é…ç½®
    # æ—§ flat key â†’ (åˆ†ç»„, å­key) çš„æ˜ å°„ï¼Œç”¨äºä¸€æ¬¡æ€§è¿ç§»æ—§ç‰ˆé…ç½®
    _LEGACY_KEY_MAP: dict[str, tuple[str, str]] = {
        "bili_video_quality": ("bili_settings", "video_quality"),
        "bili_video_codecs": ("bili_settings", "video_codecs"),
        "bili_allow_hdr": ("bili_settings", "allow_hdr"),
        "bili_allow_dolby": ("bili_settings", "allow_dolby"),
        "bili_merge_send": ("bili_settings", "merge_send"),
        "bili_enable_multi_page": ("bili_settings", "enable_multi_page"),
        "bili_multi_page_max": ("bili_settings", "multi_page_max"),
        "bili_max_duration_seconds": ("bili_settings", "max_duration_seconds"),
        "bili_allow_quality_fallback": ("bili_settings", "allow_quality_fallback"),
        "bili_cookies": ("bili_settings", "cookies"),
        "douyin_max_media": ("douyin_settings", "max_media"),
        "douyin_merge_send": ("douyin_settings", "merge_send"),
        "xhs_max_media": ("xhs_settings", "max_media"),
        "xhs_merge_send": ("xhs_settings", "merge_send"),
        "xhs_download_original": ("xhs_settings", "download_original"),
        "xhs_prefer_ci_png": ("xhs_settings", "prefer_ci_png"),
        "xhs_auto_unmerge_threshold_mb": ("xhs_settings", "auto_unmerge_threshold_mb"),
        "xhs_concurrent_download": ("xhs_settings", "concurrent_download"),
        "retry_count": ("general_settings", "retry_count"),
        "reaction_emoji_enabled": ("general_settings", "reaction_emoji_enabled"),
        "reaction_emoji_id": ("general_settings", "reaction_emoji_id"),
        "max_video_size_mb": ("general_settings", "max_video_size_mb"),
        "merge_send_as_sender": ("general_settings", "merge_send_as_sender"),
        "error_notify_mode": ("general_settings", "error_notify_mode"),
    }

    def _migrate_legacy_config(self) -> None:
        """å°†æ—§ç‰ˆ flat é…ç½®è¿ç§»åˆ°åµŒå¥—ç»“æ„ï¼Œä¿è¯é¢æ¿æ˜¾ç¤ºä¸å®é™…ç”Ÿæ•ˆä¸€è‡´ã€‚"""
        if not isinstance(self.config, dict):
            return
        migrated = False
        for old_key, (group, sub_key) in self._LEGACY_KEY_MAP.items():
            if old_key not in self.config:
                continue
            if group not in self.config or not isinstance(self.config[group], dict):
                self.config[group] = {}
            # ä»…åœ¨æ–°ä½ç½®å°šæ— å€¼æ—¶è¿ç§»
            if sub_key not in self.config[group]:
                self.config[group][sub_key] = self.config[old_key]
            del self.config[old_key]
            migrated = True
        if migrated and hasattr(self.config, "save_config"):
            self.config.save_config()
            logger.info("ğŸ“¦ å·²å°†æ—§ç‰ˆé…ç½®è¿ç§»åˆ°åˆ†ç»„ç»“æ„")

    def _get_config_value(self, key: str, default):
        keys = key.split(".")
        val = self.config
        for k in keys:
            if isinstance(val, dict):
                val = val.get(k)
            else:
                return default
            if val is None:
                return default
        return val

    def _refresh_config(self) -> None:
        # å¹³å°å¯ç”¨åˆ—è¡¨
        enable_platforms = self._get_config_value("enable_platforms", ["Bç«™", "æŠ–éŸ³", "å°çº¢ä¹¦"])
        if not isinstance(enable_platforms, list):
            enable_platforms = ["Bç«™", "æŠ–éŸ³", "å°çº¢ä¹¦"]
        self.bili_enabled = "Bç«™" in enable_platforms
        self.douyin_enabled = "æŠ–éŸ³" in enable_platforms
        self.xhs_enabled = "å°çº¢ä¹¦" in enable_platforms
        
        # Bç«™é…ç½®
        self.quality_label = str(self._get_config_value("bili_settings.video_quality", "720P"))
        self.codecs_label = str(self._get_config_value("bili_settings.video_codecs", "AVC"))
        self.allow_hdr = bool(self._get_config_value("bili_settings.allow_hdr", False))
        self.allow_dolby = bool(self._get_config_value("bili_settings.allow_dolby", False))
        self.bili_merge_send = bool(self._get_config_value("bili_settings.merge_send", False))
        self.enable_multi_page = bool(self._get_config_value("bili_settings.enable_multi_page", True))
        self.multi_page_max = max(1, int(self._get_config_value("bili_settings.multi_page_max", 3)))
        self.bili_max_duration_seconds = max(
            0, int(self._get_config_value("bili_settings.max_duration_seconds", 300))
        )
        self.allow_quality_fallback = bool(self._get_config_value("bili_settings.allow_quality_fallback", True))
        # ä»é…ç½®è¯»å– Cookie å¹¶å†™å…¥æ–‡ä»¶
        bili_cookies_str = str(self._get_config_value("bili_settings.cookies", "")).strip()
        if bili_cookies_str:
            try:
                cookies_file = get_bili_cookies_file()
                cookies_file.parent.mkdir(parents=True, exist_ok=True)
                # æ¢å¤ Netscape æ ¼å¼çš„æ¢è¡Œç¬¦ï¼ˆç½‘é¡µé…ç½®ç²˜è´´æ—¶å¯èƒ½ä¸¢å¤±ï¼‰
                if "\n" not in bili_cookies_str and ".bilibili.com" in bili_cookies_str:
                    bili_cookies_str = re.sub(
                        r"\s+(\.(?:www\.)?bilibili\.com\s)",
                        r"\n\1",
                        bili_cookies_str,
                    )
                    bili_cookies_str = bili_cookies_str.replace("# ", "\n# ").strip()
                cookies_file.write_text(bili_cookies_str, encoding="utf-8")
                logger.info("ğŸª Bç«™ Cookie å·²ä»é…ç½®å†™å…¥æ–‡ä»¶")
            except Exception as exc:
                logger.warning("âš ï¸ å†™å…¥ Bç«™ Cookie æ–‡ä»¶å¤±è´¥: %s", str(exc))
        
        # æŠ–éŸ³é…ç½®
        self.douyin_max_media = max(1, int(self._get_config_value("douyin_settings.max_media", 9)))
        self.douyin_merge_send = bool(self._get_config_value("douyin_settings.merge_send", True))
        
        # å°çº¢ä¹¦é…ç½®
        self.xhs_max_media = max(1, int(self._get_config_value("xhs_settings.max_media", 99)))
        self.xhs_merge_send = bool(self._get_config_value("xhs_settings.merge_send", True))
        self.xhs_download_original = bool(self._get_config_value("xhs_settings.download_original", True))
        self.xhs_prefer_ci_png = bool(self._get_config_value("xhs_settings.prefer_ci_png", False))
        self.xhs_auto_unmerge_threshold_mb = int(self._get_config_value("xhs_settings.auto_unmerge_threshold_mb", 50))
        self.xhs_concurrent_download = bool(self._get_config_value("xhs_settings.concurrent_download", True))
        
        # é€šç”¨é…ç½®
        self.retry_count = max(0, int(self._get_config_value("general_settings.retry_count", 3)))
        self.reaction_emoji_enabled = bool(self._get_config_value("general_settings.reaction_emoji_enabled", True))
        self.reaction_emoji_id = self._coerce_positive_int(self._get_config_value("general_settings.reaction_emoji_id", 128169), 128169)
        self.reaction_emoji_type = "1"  # å›ºå®šå€¼ï¼Œæ— éœ€é…ç½®
        self.max_video_size_mb = int(self._get_config_value("general_settings.max_video_size_mb", 200))
        self.merge_send_as_sender = bool(self._get_config_value("general_settings.merge_send_as_sender", False))
        _mode = str(self._get_config_value("general_settings.error_notify_mode", "é™é»˜")).strip()
        self.error_notify_mode = _mode if _mode in ("é™é»˜", "è„±æ•", "æŠ¥é”™") else "é™é»˜"

        alias = self._normalize_quality_alias(self.quality_label)
        if alias == "HDR":
            self.allow_hdr = True
        if alias == "DOLBY":
            self.allow_dolby = True

        self.quality_enum_name, self.video_quality = self._resolve_quality(alias)
        self.codecs_enum_name, self.video_codecs = self._resolve_codecs(self.codecs_label)

        # æ„å»ºå¯ç”¨å¹³å°åˆ—è¡¨
        enabled_list = [p for p in ["Bç«™", "æŠ–éŸ³", "å°çº¢ä¹¦"] if p in enable_platforms]
        duration_label = (
            f"{self.bili_max_duration_seconds}s"
            if self.bili_max_duration_seconds > 0
            else "æ— é™åˆ¶"
        )
        logger.info(
            "ğŸ“¹ LinkResolver é…ç½®: å¹³å°=%s, Bç«™(ç”»è´¨=%s,åˆå¹¶=%s,æ—¶é•¿<=%s), æŠ–éŸ³(åˆå¹¶=%s), å°çº¢ä¹¦(åŸå›¾=%s), é‡è¯•=%d",
            "/".join(enabled_list) if enabled_list else "æ— ",
            self.video_quality.name,
            "å¼€" if self.bili_merge_send else "å…³",
            duration_label,
            "å¼€" if self.douyin_merge_send else "å…³",
            "å¼€" if self.xhs_download_original else "å…³",
            self.retry_count,
        )
    # endregion

    # region è§£æä»»åŠ¡ç®¡ç†
    def _register_parse_task(self, kind: str, event: AstrMessageEvent | None = None) -> None:
        task = asyncio.current_task()
        if task is None:
            return
        message_id = None
        if event is not None:
            message_id = self._extract_reaction_message_id(event)
        tag = f"{kind}:{message_id or 'unknown'}"
        try:
            task.set_name(f"{TASK_NAME_PREFIX}:{tag}:{int(time.time() * 1000)}")
        except Exception:
            pass
        self._active_parse_tasks.add(task)
        task.add_done_callback(lambda t: self._active_parse_tasks.discard(t))

    def _cancel_previous_parse_tasks(self) -> None:
        """é€šè¿‡ asyncio.all_tasks() æŒ‰ä»»åŠ¡å/åç¨‹åæ‰«æå¹¶å–æ¶ˆæ—§è§£æä»»åŠ¡ã€‚

        ä¸ä¾èµ– self._active_parse_tasksï¼ˆè°ƒç”¨æ—¶è¯¥å±æ€§å°šæœªåˆå§‹åŒ–ï¼‰ã€‚
        """
        cancelled: list[str] = []
        candidates: set[asyncio.Task] = set()

        loop = None
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = None

        if loop:
            try:
                current_task = asyncio.current_task(loop=loop)
            except Exception:
                current_task = None
            try:
                tasks = asyncio.all_tasks(loop)
            except Exception:
                tasks = set()
            for task in tasks:
                if task is current_task:
                    continue
                name = task.get_name() if hasattr(task, "get_name") else ""
                if isinstance(name, str) and name.startswith(TASK_NAME_PREFIX):
                    candidates.add(task)
                    continue
                try:
                    qualname = getattr(task.get_coro(), "__qualname__", "")
                except Exception:
                    qualname = ""
                if any(
                    token in qualname
                    for token in (
                        "handle_xhs",
                        "handle_douyin",
                        "handle_bili_video",
                        "_process_xhs",
                        "_process_douyin",
                        "_process_bili_video",
                    )
                ):
                    candidates.add(task)

        for task in candidates:
            if task.done():
                continue
            try:
                task.cancel()
                name = task.get_name() if hasattr(task, "get_name") else ""
                if name:
                    cancelled.append(name)
            except Exception:
                continue

        if cancelled:
            sample = ", ".join(cancelled[:5])
            suffix = "..." if len(cancelled) > 5 else ""
            logger.info("â™»ï¸ æ’ä»¶é‡è½½ï¼Œå·²ä¸­æ–­æ—§è§£æä»»åŠ¡ %d ä¸ª: %s%s", len(cancelled), sample, suffix)
        else:
            logger.info("â™»ï¸ æ’ä»¶é‡è½½ï¼Œæœªå‘ç°å¯ä¸­æ–­çš„æ—§è§£æä»»åŠ¡")
    # endregion

    # region é€šç”¨å·¥å…·
    def _has_json_component(self, event: AstrMessageEvent) -> bool:
        if not hasattr(event, "message_obj") or not hasattr(event.message_obj, "message"):
            return False
        for component in event.message_obj.message:
            if isinstance(component, dict):
                comp_type = component.get("type")
                if comp_type == "reply":
                    continue
                if comp_type and "json" in str(comp_type).lower():
                    return True
                continue
            if isinstance(component, Comp.Json):
                return True
            comp_type = getattr(component, "type", None)
            if comp_type and "json" in str(comp_type).lower():
                return True
        return False

    @staticmethod
    def _coerce_positive_int(value: object, default: int) -> int:
        if value is None:
            return default
        if isinstance(value, bool):
            return default
        try:
            if isinstance(value, (int, float)):
                parsed = int(value)
                return parsed if parsed > 0 else default
            text = str(value).strip()
            if text.isdigit():
                parsed = int(text)
                return parsed if parsed > 0 else default
        except Exception:
            return default
        return default

    @staticmethod
    def _format_duration(duration_seconds: int | None) -> str | None:
        if not duration_seconds:
            return None
        minutes = int(duration_seconds) // 60
        seconds = int(duration_seconds) % 60
        return f"{minutes}:{seconds:02d}"

    @staticmethod
    def _hash_url(url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()[:16]

    @staticmethod
    def _guess_media_suffix(url: str, default: str) -> str:
        try:
            suffix = Path(urlparse(url).path).suffix
        except Exception:
            suffix = ""
        if suffix and len(suffix) <= 5:
            return suffix
        return default

    # endregion

    # region é“¾æ¥æå–
    @staticmethod
    def _extract_urls_from_text(text: str) -> list[str]:
        if not text:
            return []
        return re.findall(r"https?://[^\s'\"<>]+", text)

    def _coerce_json_payload(self, json_component) -> dict | None:
        def unwrap(value, depth: int = 0) -> dict | None:
            if depth > 4 or value is None:
                return None
            if isinstance(value, str):
                value = value.strip()
                if not value:
                    return None
                try:
                    return unwrap(json.loads(value), depth + 1)
                except Exception:
                    return None
            if isinstance(value, dict):
                if any(key in value for key in ("meta", "prompt", "ver", "app", "view", "config")):
                    return value
                if "data" in value:
                    return unwrap(value["data"], depth + 1)
                return value
            if isinstance(value, list):
                for item in value:
                    payload = unwrap(item, depth + 1)
                    if payload:
                        return payload
            return None

        if hasattr(json_component, "data"):
            return unwrap(json_component.data)
        return unwrap(json_component)

    def extract_links_from_json(self, json_component) -> list[str]:
        links: list[str] = []
        try:
            json_data = self._coerce_json_payload(json_component)
            if not json_data:
                return links

            def search_json_for_links(obj):
                found: list[str] = []
                if isinstance(obj, dict):
                    for value in obj.values():
                        if isinstance(value, str):
                            found.extend(self._extract_urls_from_text(value))
                        elif isinstance(value, (dict, list)):
                            found.extend(search_json_for_links(value))
                elif isinstance(obj, list):
                    for item in obj:
                        if isinstance(item, str):
                            found.extend(self._extract_urls_from_text(item))
                        elif isinstance(item, (dict, list)):
                            found.extend(search_json_for_links(item))
                return found

            links.extend(search_json_for_links(json_data))

            if isinstance(json_data, dict):
                meta = json_data.get("meta", {})
                detail = meta.get("detail_1", {}) if meta else {}
                if detail:
                    for key in ("qqdocurl", "url"):
                        value = detail.get(key, "")
                        if value:
                            links.extend(self._extract_urls_from_text(value))
        except Exception as exc:
            logger.warning("âš ï¸ è§£æ JSON æ¶ˆæ¯ç»„ä»¶å¤±è´¥: %s", str(exc))
        return links
    # endregion

    # region æ¶ˆæ¯åŸºç¡€åˆ¤æ–­
    @staticmethod
    def _is_self_message(event: AstrMessageEvent) -> bool:
        try:
            return str(event.get_sender_id()) == str(event.get_self_id())
        except Exception:
            return False

    async def _is_bot_muted(self, event: AstrMessageEvent) -> bool:
        """æ£€æµ‹ Bot æ˜¯å¦åœ¨ç¾¤ä¸­è¢«ç¦è¨€ã€‚
        
        é€šè¿‡ OneBot V11 çš„ get_group_member_info API è·å– Bot åœ¨ç¾¤ä¸­çš„ä¿¡æ¯ï¼Œ
        æ£€æŸ¥ shut_up_timestamp å­—æ®µåˆ¤æ–­æ˜¯å¦è¢«ç¦è¨€ã€‚
        
        Returns:
            True å¦‚æœ Bot è¢«ç¦è¨€ï¼ŒFalse å¦‚æœæœªè¢«ç¦è¨€æˆ–æ— æ³•æ£€æµ‹ã€‚
        """
        group_id = event.get_group_id()
        if not group_id:
            return False
        
        bot = getattr(event, "bot", None)
        if bot is None or not hasattr(bot, "call_action"):
            return False
        
        self_id = event.get_self_id()
        if not self_id:
            return False
        
        try:
            member_info = await bot.call_action(
                "get_group_member_info",
                group_id=int(group_id),
                user_id=int(self_id),
                no_cache=True,
            )
            shut_up_timestamp = member_info.get("shut_up_timestamp", 0)
            if shut_up_timestamp and shut_up_timestamp > time.time():
                logger.info("ğŸ”‡ Bot åœ¨ç¾¤ %s ä¸­è¢«ç¦è¨€ï¼Œè·³è¿‡å¤„ç†", group_id)
                return True
        except Exception as exc:
            logger.debug("æ£€æµ‹ç¦è¨€çŠ¶æ€å¤±è´¥: %s", str(exc))
        
        return False
    # endregion

    # region è¡¨æƒ…å›åº”
    def _extract_reaction_message_id(self, event: AstrMessageEvent) -> int | None:
        raw = getattr(event.message_obj, "raw_message", None)
        candidates: list[object] = []
        if isinstance(raw, dict):
            candidates.append(raw.get("message_id"))
        elif raw is not None and hasattr(raw, "message_id"):
            candidates.append(getattr(raw, "message_id", None))
        candidates.append(getattr(event.message_obj, "message_id", None))
        for value in candidates:
            if value is None:
                continue
            try:
                mid = int(value)
            except Exception:
                continue
            if mid > 0:
                return mid
        return None

    async def _send_reaction_emoji(self, event: AstrMessageEvent, source_tag: str) -> None:
        if not self.reaction_emoji_enabled:
            return
        if not event.get_group_id():
            logger.debug("è¡¨æƒ…å›åº”è·³è¿‡%s: éç¾¤æ¶ˆæ¯", source_tag)
            return
        bot = getattr(event, "bot", None)
        if bot is None or not hasattr(bot, "set_msg_emoji_like"):
            logger.debug("è¡¨æƒ…å›åº”è·³è¿‡%s: å¹³å°ä¸æ”¯æŒ", source_tag)
            return
        message_id = self._extract_reaction_message_id(event)
        if message_id is None:
            logger.debug("è¡¨æƒ…å›åº”è·³è¿‡%s: æ— æ³•è·å– message_id", source_tag)
            return
        try:
            await bot.set_msg_emoji_like(
                message_id=message_id,
                emoji_id=self.reaction_emoji_id,
                emoji_type=self.reaction_emoji_type,
                set=True,
            )
        except Exception as exc:
            logger.warning("âš ï¸ è¡¨æƒ…å›åº”å¤±è´¥%s: %s", source_tag, str(exc))
    # endregion

    # region ä¸‹è½½å·¥å…·
    async def _probe_stream_size(
        self,
        url: str,
        cookies: dict[str, str] | None = None,
        headers: dict[str, str] | None = None,
    ) -> int | None:
        try:
            headers = headers or {}
            cookies = cookies or {}
            async with httpx.AsyncClient(timeout=10.0, headers=headers, cookies=cookies) as client:
                response = await client.head(url, follow_redirects=True)
                if response.status_code >= 400:
                    return None
                length = response.headers.get("Content-Length")
                if length:
                    return int(length)
                range_headers = {**headers, "Range": "bytes=0-0"}
                response = await client.get(url, headers=range_headers)
                content_range = response.headers.get("Content-Range", "")
                if "/" in content_range:
                    return int(content_range.split("/")[-1])
        except asyncio.CancelledError:
            raise
        except Exception:
            return None
        return None

    async def _estimate_total_size_mb(
        self,
        video_url: str,
        audio_url: str | None,
        cookies: dict[str, str] | None = None,
        headers: dict[str, str] | None = None,
    ) -> float | None:
        total = 0
        unknown = False
        for url in (video_url, audio_url):
            if not url:
                continue
            size = await self._probe_stream_size(url, cookies=cookies, headers=headers)
            if size is None:
                unknown = True
                continue
            total += size
        if total == 0 and unknown:
            return None
        return total / 1024 / 1024

    async def _download_stream(
        self,
        url: str,
        output_path: Path,
        cookies: dict[str, str] | None,
        max_bytes: int | None,
        headers: dict[str, str] | None = None,
        retries: int = 3,
    ) -> int:
        temp_path = output_path.with_suffix(output_path.suffix + ".part")
        last_error: Exception | None = None
        for attempt in range(retries):
            try:
                hdrs = headers or {}
                cks = cookies or {}
                async with httpx.AsyncClient(timeout=None, headers=hdrs, cookies=cks) as client:
                    async with client.stream("GET", url, follow_redirects=True) as response:
                        response.raise_for_status()
                        content_length = response.headers.get("Content-Length")
                        if content_length and max_bytes and int(content_length) > max_bytes:
                            raise SizeLimitExceeded("è¶…è¿‡å¤§å°é™åˆ¶")
                        bytes_written = 0
                
                        # Actually, wrapping each write is fine if chunks are large
                        with open(temp_path, "wb") as file:
                            async for chunk in response.aiter_bytes(1024 * 1024):
                                if not chunk:
                                    continue
                                bytes_written += len(chunk)
                                if max_bytes and bytes_written > max_bytes:
                                    raise SizeLimitExceeded("è¶…è¿‡å¤§å°é™åˆ¶")
                                await asyncio.to_thread(file.write, chunk)
                await asyncio.to_thread(temp_path.replace, output_path)
                return bytes_written
            except asyncio.CancelledError:
                if temp_path.exists():
                    await asyncio.to_thread(temp_path.unlink, missing_ok=True)
                raise
            except SizeLimitExceeded:
                if temp_path.exists():
                    await asyncio.to_thread(temp_path.unlink, missing_ok=True)
                raise
            except Exception as exc:
                last_error = exc
                if temp_path.exists():
                    await asyncio.to_thread(temp_path.unlink, missing_ok=True)
                if attempt < retries - 1:
                    wait_time = 2 ** attempt
                    logger.warning("âš ï¸ ä¸‹è½½å¤±è´¥, %dç§’åé‡è¯• (%d/%d): %s", wait_time, attempt + 1, retries, str(exc))
                    await asyncio.sleep(wait_time)
        if last_error:
            raise last_error
        raise RuntimeError("ä¸‹è½½å¤±è´¥")

    async def _merge_av(self, v_path: Path, a_path: Path, output_path: Path) -> None:
        cmd = [
            "ffmpeg",
            "-y",
            "-i",
            str(v_path),
            "-i",
            str(a_path),
            "-c",
            "copy",
            "-map",
            "0:v:0",
            "-map",
            "1:a:0",
            str(output_path),
        ]
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
            )
            _, stderr = await process.communicate()
            if process.returncode != 0:
                raise RuntimeError(stderr.decode().strip())
        finally:
            await asyncio.to_thread(v_path.unlink, missing_ok=True)
            await asyncio.to_thread(a_path.unlink, missing_ok=True)

    async def download_thumbnail(self, url: str, save_path: Path) -> bool:
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(url)
                if response.status_code == 200:
                    await asyncio.to_thread(save_path.write_bytes, response.content)
                    return True
        except asyncio.CancelledError:
            raise
        except Exception as exc:
            logger.warning("âš ï¸ ä¸‹è½½å°é¢å¤±è´¥: %s", str(exc))
        return False

    async def calculate_md5(self, file_path: Path) -> str:
        def _sync_md5():
            hasher = hashlib.md5()
            with open(file_path, "rb") as file:
                while chunk := file.read(8192):
                    hasher.update(chunk)
            return hasher.hexdigest()
        return await asyncio.to_thread(_sync_md5)

    async def cleanup_files(self, video_paths: list[Path], thumbnail_paths: list[Path]) -> None:
        # Direct Send Pattern: è°ƒç”¨æ­¤æ–¹æ³•æ—¶ï¼Œæ–‡ä»¶å·²é€šè¿‡ await event.send() è¢«è¯»å–å®Œæ¯•
        # æ— éœ€å»¶è¿Ÿï¼Œç«‹å³æ¸…ç†ä»¥é¿å…ä¸åç»­ç›¸åŒ URL è¯·æ±‚äº§ç”Ÿç«æ€æ¡ä»¶
        for video_path in video_paths:
            existed = await asyncio.to_thread(video_path.exists)
            await asyncio.to_thread(video_path.unlink, missing_ok=True)
            logger.debug("ğŸ§¹ æ¸…ç†è§†é¢‘æ–‡ä»¶: path=%s, existed=%s", video_path, existed)
        for thumb_path in thumbnail_paths:
            existed = await asyncio.to_thread(thumb_path.exists)
            await asyncio.to_thread(thumb_path.unlink, missing_ok=True)
            logger.debug("ğŸ§¹ æ¸…ç†ç¼©ç•¥å›¾æ–‡ä»¶: path=%s, existed=%s", thumb_path, existed)
    # endregion

    # region åˆå¹¶è½¬å‘å‘é€äººè·å–
    def _get_merge_sender_uin(self, event: AstrMessageEvent) -> str:
        """è·å–åˆå¹¶è½¬å‘ä½¿ç”¨çš„ uin

        æ ¹æ® merge_send_as_sender é…ç½®å†³å®šä½¿ç”¨å‘é€è€…çš„ uin è¿˜æ˜¯ Bot çš„ uin
        """
        if self.merge_send_as_sender:
            sender_id = event.get_sender_id()
            if sender_id:
                return str(sender_id)
        return str(event.get_self_id())
    # endregion




    # region äº‹ä»¶å¤„ç†å™¨
    @filter.regex(BILI_MESSAGE_PATTERN, priority=10)
    async def handle_bili_video(self, event: AstrMessageEvent):
        if self._has_json_component(event):
            return
        self._register_parse_task("bili", event)
        await BilibiliMixin.handle_bili_video(self, event)

    @filter.regex(DOUYIN_MESSAGE_PATTERN, priority=10)
    async def handle_douyin(self, event: AstrMessageEvent):
        if self._has_json_component(event):
            return
        self._register_parse_task("douyin", event)
        await DouyinMixin.handle_douyin(self, event)

    @filter.regex(XHS_MESSAGE_PATTERN, priority=10)
    async def handle_xhs(self, event: AstrMessageEvent):
        if self._has_json_component(event):
            return
        self._register_parse_task("xhs", event)
        await XiaohongshuMixin.handle_xhs(self, event)

    @filter.regex(r".*")
    async def handle_json_card(self, event: AstrMessageEvent):
        if self._is_self_message(event):
            return

        links: list[str] = []
        has_json_component = False
        if hasattr(event, "message_obj") and hasattr(event.message_obj, "message"):
            for component in event.message_obj.message:
                is_json_component = False
                comp_payload = component
                if isinstance(component, dict):
                    comp_type = component.get("type")
                    if comp_type == "reply": # å¿½ç•¥å¼•ç”¨å›å¤ç»„ä»¶ï¼Œé˜²æ­¢å›å¤æ—¶é€’å½’è§£æåŸæ¶ˆæ¯
                        continue
                    comp_payload = component.get("data") or component
                    is_json_component = bool(comp_type) and "json" in str(comp_type).lower()
                else:
                    if isinstance(component, Comp.Json):
                        is_json_component = True
                    comp_type = getattr(component, "type", None)
                    if not is_json_component and comp_type:
                        is_json_component = "json" in str(comp_type).lower()
                    if is_json_component and hasattr(component, "data"):
                        comp_payload = component.data
                if is_json_component:
                    has_json_component = True
                    logger.info("ğŸ”— æ£€æµ‹åˆ° JSON å¡ç‰‡æ¶ˆæ¯: %s", component)
                    links.extend(self.extract_links_from_json(comp_payload))
        if not has_json_component:
            return

        if await self._is_bot_muted(event):
            return

        if not links:
            return
        unique_links = list(dict.fromkeys(links))
        bili_links = [link for link in unique_links if re.search(BILI_MESSAGE_PATTERN, link)]
        douyin_links = [link for link in unique_links if re.search(DOUYIN_MESSAGE_PATTERN, link)]
        xhs_links = [link for link in unique_links if re.search(XHS_MESSAGE_PATTERN, link)]

        if bili_links and self.bili_enabled:
            self._register_parse_task("json-bili", event)
            event.should_call_llm(True)
            try:
                ref = await self._resolve_video_ref_from_links(bili_links)
                if ref:
                    await self._process_bili_video(event, ref=ref, is_from_card=True)
                    return
                logger.warning("âš ï¸ ä»å¡ç‰‡ä¸­æ‰¾åˆ° B ç«™é“¾æ¥ä½†æ— æ³•è§£æ: %s", bili_links)
            except asyncio.CancelledError:
                logger.info("â™»ï¸ JSONå¡ç‰‡è§£æä»»åŠ¡å·²ä¸­æ–­")
                return

        if douyin_links and self.douyin_enabled:
            self._register_parse_task("json-douyin", event)
            event.should_call_llm(True)
            try:
                await self._process_douyin(event, douyin_links[0], is_from_card=True)
                return
            except asyncio.CancelledError:
                logger.info("â™»ï¸ JSONå¡ç‰‡è§£æä»»åŠ¡å·²ä¸­æ–­")
                return

        if xhs_links and self.xhs_enabled:
            self._register_parse_task("json-xhs", event)
            event.should_call_llm(True)
            try:
                await self._process_xhs(event, xhs_links[0], is_from_card=True)
                return
            except asyncio.CancelledError:
                logger.info("â™»ï¸ JSONå¡ç‰‡è§£æä»»åŠ¡å·²ä¸­æ–­")
                return

        logger.warning("âš ï¸ ä»å¡ç‰‡ä¸­æ‰¾åˆ°é“¾æ¥ä½†æ— æ³•è§£æ: %s", unique_links)
    # endregion
# endregion
