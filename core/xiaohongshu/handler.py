# region å¯¼å…¥
import asyncio
import re
import uuid
from pathlib import Path
from urllib.parse import urlparse

import aiohttp
from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent, MessageChain
from astrbot.api.message_components import Image, Node, Nodes, Video

from ..common import (
    CHROME_UA,
    SizeLimitExceeded,
    get_xhs_video_path,
    get_xhs_image_path,
    get_xhs_card_path,
)
from . import (
    XHS_HEADERS,
    XHS_MESSAGE_PATTERN,
    XiaohongshuParseError,
    XiaohongshuRetryableError,
    XiaohongshuResult,
    extract_xhs_links,
    load_xhs_cookies,
)
# endregion

# region è§£æç­–ç•¥å¸¸é‡
XHS_PARSE_TIMEOUT_SEC = 30.0
XHS_PARSE_RETRY_BASE_DELAY_SEC = 1.0
XHS_PARSE_RETRY_MAX_DELAY_SEC = 8.0
# endregion


# region å°çº¢ä¹¦æ··å…¥
class XiaohongshuMixin:
    # region è·¯å¾„ä¸å€™é€‰æ„å»º
    def _build_xhs_path(self, url: str, is_video: bool, request_id: str) -> Path:
        suffix = ".mp4" if is_video else self._guess_media_suffix(url, ".jpg")
        base_dir = get_xhs_video_path() if is_video else get_xhs_image_path()
        return base_dir / f"{self._hash_url(url)}_{request_id}{suffix}"

    def _build_xhs_card_path(self, source_url: str, request_id: str) -> Path:
        return get_xhs_card_path() / f"{self._hash_url(source_url)}_{request_id}_card.png"

    @staticmethod
    def _force_https(url: str) -> str:
        try:
            parsed = urlparse(url)
        except Exception:
            return url
        if parsed.scheme in ("", "http"):
            return parsed._replace(scheme="https").geturl()
        return url
    # endregion

    # region ä¸‹è½½ä¸æ¸²æŸ“
    @staticmethod
    def _xhs_download_headers(referer: str | None) -> dict[str, str]:
        headers = dict(XHS_HEADERS)
        if referer:
            headers["Referer"] = referer
        headers["Origin"] = "https://www.xiaohongshu.com"
        headers["Accept"] = "image/avif,image/webp,image/apng,image/*,*/*;q=0.8"
        headers["Accept-Language"] = "zh-CN,zh;q=0.9"
        return headers

    def _get_xhs_cookies(self) -> dict[str, str]:
        """è·å–å°çº¢ä¹¦ cookiesï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨ï¼‰"""
        # æ£€æŸ¥é…ç½®æ˜¯å¦å¯ç”¨ cookies
        if not getattr(self, 'xhs_use_cookies', False):
            return {}
        if not hasattr(self, "_xhs_cookies_cache"):
            self._xhs_cookies_cache = load_xhs_cookies()
        return self._xhs_cookies_cache

    def _try_extend_aiocqhttp_timeout(self, event: AstrMessageEvent, timeout_sec: float) -> None:
        """å°è¯•å»¶é•¿ aiocqhttp API è¶…æ—¶æ—¶é—´ï¼ˆç”¨äºå‘é€å¤§å›¾ï¼‰
        
        è·¯å¾„: event.bot._api._wsr_api._timeout_sec
        """
        try:
            bot = getattr(event, 'bot', None)
            if not bot:
                return
            api = getattr(bot, '_api', None)
            if not api:
                return
            wsr_api = getattr(api, '_wsr_api', None)
            if not wsr_api:
                return
            current_timeout = getattr(wsr_api, '_timeout_sec', 180)
            if current_timeout < timeout_sec:
                wsr_api._timeout_sec = timeout_sec
                logger.debug("aiocqhttp API è¶…æ—¶å·²ä» %.0fs å»¶é•¿åˆ° %.0fs", current_timeout, timeout_sec)
        except Exception as e:
            logger.debug("æ— æ³•ä¿®æ”¹ aiocqhttp è¶…æ—¶: %s", str(e))

    @staticmethod
    def _is_retryable_xhs_exception(exc: Exception) -> bool:
        if isinstance(exc, (asyncio.TimeoutError, XiaohongshuRetryableError)):
            return True
        text = str(exc).lower()
        retryable_patterns = (
            "timeout",
            "timed out",
            "connection",
            "reset",
            "refused",
            "temporary",
            "unavailable",
            "503",
            "502",
            "504",
            "429",
            "network",
        )
        return any(p in text for p in retryable_patterns)

    async def _download_xhs_video(self, url: str, request_id: str, referer: str | None = None) -> Path:
        max_bytes = self.max_video_size_mb * 1024 * 1024 if self.max_video_size_mb > 0 else None
        cookies = self._get_xhs_cookies()
        size_mb = await self._estimate_total_size_mb(
            url, None, headers=self._xhs_download_headers(referer), cookies=cookies
        )
        logger.info(
            "ğŸ“¹ ä¼°ç®—å°çº¢ä¹¦è§†é¢‘å¤§å°: %s MB",
            f"{size_mb:.2f}" if size_mb is not None else "æœªçŸ¥",
        )
        if size_mb is not None and max_bytes and size_mb * 1024 * 1024 > max_bytes:
            raise SizeLimitExceeded("è¶…è¿‡å¤§å°é™åˆ¶")
        output_path = self._build_xhs_path(url, is_video=True, request_id=request_id)
        await self._download_stream(
            url,
            output_path,
            cookies=cookies,
            max_bytes=max_bytes,
            headers=self._xhs_download_headers(referer),
            retries=3,
        )
        return output_path

    async def _download_xhs_image(
        self,
        url: str,
        request_id: str,
        file_id: str | None = None,
        referer: str | None = None
    ) -> Path:
        """ä¸‹è½½å›¾ç‰‡ - ä¸‰çº§å›é€€ç­–ç•¥
        
        1. å¦‚æœå¼€å¯åŸå›¾ä¸‹è½½ï¼šå°è¯• PNG åŸå›¾ (ci.xiaohongshu.com)
        2. å¦‚æœ PNG å¤±è´¥ï¼šå°è¯• JPEG åŸå›¾
        3. å¦‚æœéƒ½å¤±è´¥ï¼šå›é€€åˆ°å¤š CDN å…œåº•ç­–ç•¥
        """
        import time as time_module
        start_time = time_module.perf_counter()
        
        output_path = self._build_xhs_path(url, is_video=False, request_id=request_id)
        cookies = self._get_xhs_cookies()
        
        # æå– image token (å‚è€ƒ XHS-Downloader)
        token = self._extract_image_token(url)
        logger.debug("XHS å›¾ç‰‡ä¸‹è½½å¼€å§‹: url=%s, file_id=%s, token=%s", url[:80], file_id, token)
        
        # region åŸå›¾ä¸‹è½½å°è¯•
        if getattr(self, 'xhs_download_original', True) and token:
            original_start = time_module.perf_counter()
            
            # æ„å»ºåŸå›¾ URL å€™é€‰åˆ—è¡¨
            # ç­–ç•¥ï¼šä¼˜å…ˆ imageView2/format/png è·å–æ— æŸ PNG åŸå›¾ï¼ˆXHS-Downloader é»˜è®¤æ¨¡å¼ï¼‰
            #       PNG å¤±è´¥åå†å°è¯•ç›´æ¥ CDNï¼ˆauto æ¨¡å¼ï¼Œå¯èƒ½è¿”å›å‹ç¼©çš„ JPEG/WebPï¼‰
            original_candidates = []
            
            # 1. imageView2 æ ¼å¼è½¬æ¢ - PNG ä¼˜å…ˆï¼ˆXHS-Downloader é»˜è®¤ä½¿ç”¨ png æ ¼å¼ï¼‰
            #    ci.xiaohongshu.com ä¼šå°†å›¾ç‰‡è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ï¼ŒPNG é€šå¸¸æ˜¯æ— æŸçš„æœ€å¤§å°ºå¯¸
            original_candidates.append({
                "url": f"https://ci.xiaohongshu.com/{token}?imageView2/format/png",
                "desc": "CI-PNG-åŸå›¾",
                "format": "png",
            })
            
            # 2. ç›´æ¥ CDN é“¾æ¥ - auto æ¨¡å¼ï¼ˆå¯èƒ½è¿”å›å‹ç¼©çš„ JPEG/WebPï¼‰
            cdn_domains = [
                "sns-img-bd.xhscdn.com",  # XHS-Downloader çš„ auto æ¨¡å¼ä½¿ç”¨
                "sns-img-qc.xhscdn.com",
                "sns-img-hw.xhscdn.com",
            ]
            for domain in cdn_domains:
                original_candidates.append({
                    "url": f"https://{domain}/{token}",
                    "desc": f"CDN-{domain.split('-')[2].split('.')[0]}-auto",
                    "format": None,  # ä¿æŒåŸå§‹æ ¼å¼ï¼ˆå¯èƒ½æ˜¯å‹ç¼©æ ¼å¼ï¼‰
                })
            
            # 3. å…¶ä»–æ ¼å¼ä½œä¸ºæœ€åå¤‡é€‰
            original_candidates.append({
                "url": f"https://ci.xiaohongshu.com/{token}?imageView2/format/jpeg",
                "desc": "CI-JPEG",
                "format": "jpeg",
            })
            
            retry_count = max(0, int(getattr(self, "retry_count", 3)))
            for cand in original_candidates:
                cand_url = cand["url"]
                desc = cand["desc"]
                format_name = cand["format"]
                
                for attempt in range(retry_count + 1):
                    attempt_start = time_module.perf_counter()
                    try:
                        timeout = aiohttp.ClientTimeout(total=600, connect=60)
                        headers = {
                            "User-Agent": CHROME_UA,
                            "Referer": "https://www.xiaohongshu.com/",
                        }
                        
                        async with aiohttp.ClientSession(
                            headers=headers,
                            cookies=cookies if cookies else None,
                            timeout=timeout
                        ) as session:
                            async with session.get(cand_url) as resp:
                                attempt_elapsed = time_module.perf_counter() - attempt_start
                                
                                if resp.status == 200:
                                    # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–å¯¼è‡´ payload ä¸å®Œæ•´
                                    temp_output = output_path.with_suffix(".tmp")
                                    temp_path = temp_output.with_suffix(temp_output.suffix + ".part")
                                    content_len = 0
                                    f = None
                                    try:
                                        def _open_temp():
                                            temp_path.parent.mkdir(parents=True, exist_ok=True)
                                            return open(temp_path, "wb")

                                        f = await asyncio.to_thread(_open_temp)
                                        try:
                                            async for chunk in resp.content.iter_chunked(256 * 1024):
                                                if not chunk:
                                                    continue
                                                content_len += len(chunk)
                                                await asyncio.to_thread(f.write, chunk)
                                        finally:
                                            if f is not None:
                                                await asyncio.to_thread(f.close)

                                        # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆè‡³å°‘ 10KB æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆå›¾ç‰‡ï¼‰
                                        if content_len >= 10 * 1024 and temp_path.exists():
                                            # ç¡®å®šè¾“å‡ºæ–‡ä»¶åç¼€
                                            if format_name:
                                                actual_suffix = f".{format_name}"
                                            else:
                                                def _read_head():
                                                    with open(temp_path, "rb") as rf:
                                                        return rf.read(32)
                                                head = await asyncio.to_thread(_read_head)
                                                actual_suffix = self._detect_image_suffix(head, resp.headers.get("Content-Type"))
                                            
                                            final_output = output_path.with_suffix(actual_suffix)
                                            final_part = final_output.with_suffix(final_output.suffix + ".part")
                                            def _move():
                                                if final_part.exists():
                                                    final_part.unlink()
                                                temp_path.replace(final_part)
                                                final_part.replace(final_output)
                                            await asyncio.to_thread(_move)
                                            
                                            total_elapsed = time_module.perf_counter() - start_time
                                            logger.debug(
                                                "XHS åŸå›¾ä¸‹è½½æˆåŠŸ (%s): size=%.1fMB, è¯·æ±‚è€—æ—¶=%.2fs, æ€»è€—æ—¶=%.2fs",
                                                desc, content_len / 1024 / 1024, attempt_elapsed, total_elapsed
                                            )
                                            return final_output
                                        else:
                                            logger.debug(
                                                "XHS åŸå›¾å“åº”è¿‡å° (%s): size=%d bytes, è€—æ—¶=%.2fs",
                                                desc, content_len, attempt_elapsed
                                            )
                                    finally:
                                        if temp_path.exists() and content_len < 10 * 1024:
                                            try:
                                                temp_path.unlink()
                                            except Exception:
                                                pass
                                else:
                                    logger.debug(
                                        "XHS åŸå›¾è¯·æ±‚å¤±è´¥ (%s): HTTP %d, è€—æ—¶=%.2fs",
                                        desc, resp.status, attempt_elapsed
                                    )
                    except asyncio.CancelledError:
                        raise
                    except Exception as e:
                        attempt_elapsed = time_module.perf_counter() - attempt_start
                        logger.debug(
                            "XHS åŸå›¾ä¸‹è½½å¼‚å¸¸ (%s): %s, è€—æ—¶=%.2fs",
                            desc, str(e)[:50], attempt_elapsed
                        )

                    if attempt < retry_count:
                        wait_time = 0.5 * (2 ** attempt)
                        await asyncio.sleep(wait_time)
            
            original_elapsed = time_module.perf_counter() - original_start
            logger.debug("XHS åŸå›¾ä¸‹è½½å…¨éƒ¨å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šç­–ç•¥ï¼ŒåŸå›¾å°è¯•è€—æ—¶=%.2fs", original_elapsed)
        # endregion
        
        # region CDN å…œåº•ç­–ç•¥
        fallback_start = time_module.perf_counter()
        
        # åŸºç¡€ Headers
        base_headers = {
            "User-Agent": CHROME_UA,
        }
        
        # æ„å»ºé˜¶æ¢¯å€™é€‰åˆ—è¡¨
        candidates = []
        
        # 1. åŸå§‹ URL (å¸¦ç­¾å)
        raw_url = url.replace("http://", "https://", 1) if url.startswith("http://") else url
        candidates.append({"url": raw_url, "use_cookies": False, "desc": "Raw-NoCookie"})
        candidates.append({"url": raw_url, "use_cookies": True, "desc": "Raw-WithCookie"})

        effective_id = file_id or token
        if effective_id:
            # 2. æ— ç­¾åé€šç”¨ CDN (å…œåº•æ–¹æ¡ˆ)
            domains = [
                "sns-img-bd.xhscdn.com",
                "sns-img-qc.xhscdn.com", 
                "sns-img-hw.xhscdn.com", 
                "sns-webpic-qc.xhscdn.com",
            ]
            for domain in domains:
                for path_prefix in ["", "spectrum/"]:
                    path = f"{path_prefix}{effective_id}"
                    candidates.append({"url": f"https://{domain}/{path}", "use_cookies": False, "desc": f"CDN-{domain.split('.')[0]}"})
        
        errors = []
        retry_count = max(0, int(getattr(self, "retry_count", 3)))
        for cand in candidates:
            cand_url = cand["url"]
            use_cookies_flag = cand["use_cookies"]
            desc = cand["desc"]
            
            # ä¸¤ç§ header å˜ä½“
            header_variants = [
                {**base_headers, "Referer": "https://www.xiaohongshu.com/"},
                base_headers.copy(),
            ]
            
            for hv in header_variants:
                for attempt in range(retry_count + 1):
                    attempt_start = time_module.perf_counter()
                    try:
                        # è¶…é•¿è¶…æ—¶
                        timeout = aiohttp.ClientTimeout(total=300, connect=30)
                        async with aiohttp.ClientSession(
                            headers=hv, 
                            cookies=cookies if use_cookies_flag else None,
                            timeout=timeout
                        ) as session:
                            async with session.get(cand_url) as resp:
                                if resp.status == 200:
                                    content = await resp.read()
                                    if len(content) >= 1024:
                                        temp_path = output_path.with_suffix(output_path.suffix + ".part")
                                        def _save_fallback():
                                            with open(temp_path, "wb") as f:
                                                f.write(content)
                                            if temp_path.exists():
                                                temp_path.replace(output_path)
                                        await asyncio.to_thread(_save_fallback)
                                        
                                        attempt_elapsed = time_module.perf_counter() - attempt_start
                                        total_elapsed = time_module.perf_counter() - start_time
                                        logger.info(
                                            "XHS CDN å›¾ç‰‡ä¸‹è½½æˆåŠŸ (%s): size=%.1fKB, è¯·æ±‚è€—æ—¶=%.2fs, æ€»è€—æ—¶=%.2fs",
                                            desc, len(content) / 1024, attempt_elapsed, total_elapsed
                                        )
                                        return output_path
                                
                                errors.append(f"{desc}: HTTP {resp.status}")
                    except asyncio.CancelledError:
                        raise
                    except Exception as e:
                        errors.append(f"{desc}: {str(e)[:20]}")

                    if attempt < retry_count:
                        wait_time = 0.5 * (2 ** attempt)
                        await asyncio.sleep(wait_time)
        # endregion
        
        # å…¨éƒ¨å¤±è´¥
        total_elapsed = time_module.perf_counter() - start_time
        error_summary = " | ".join(errors[:5])  # åªå–å‰5ä¸ªé”™è¯¯
        logger.error("XHS å›¾ç‰‡ä¸‹è½½å…¨çº¿å¤±è´¥: æ€»è€—æ—¶=%.2fs, é”™è¯¯=%s", total_elapsed, error_summary)
        raise RuntimeError(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {error_summary}")
    
    @staticmethod
    def _extract_image_token(url: str) -> str | None:
        """ä» URL ä¸­æå– image tokenï¼ˆå‚è€ƒ XHS-Downloaderï¼‰"""
        if not url:
            return None
        try:
            # æ ¼å¼: https://xxx.xhscdn.com/spectrum/1040g0k... æˆ–ç±»ä¼¼
            # æå–è·¯å¾„ç¬¬5ä¸ª/ä¹‹åçš„éƒ¨åˆ†ï¼Œå»æ‰!åç¼€
            parts = url.split("/")
            if len(parts) >= 6:
                token = "/".join(parts[5:]).split("!")[0].split("?")[0]
                if token and len(token) > 10:
                    return token
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥å–æœ€åä¸€æ®µ
            last_part = url.split("/")[-1].split("!")[0].split("?")[0]
            if last_part and len(last_part) > 10:
                return last_part
        except Exception:
            pass
        return None
    
    @staticmethod
    def _detect_image_suffix(content: bytes, content_type: str | None) -> str:
        """ä»æ–‡ä»¶ç­¾åæˆ– Content-Type æ£€æµ‹å›¾ç‰‡æ ¼å¼"""
        # æ–‡ä»¶ç­¾åæ£€æµ‹ï¼ˆé­”æ•°ï¼‰
        if content[:8] == b'\x89PNG\r\n\x1a\n':
            return ".png"
        if content[:3] == b'\xff\xd8\xff':
            return ".jpeg"
        if content[:4] == b'RIFF' and content[8:12] == b'WEBP':
            return ".webp"
        if content[:4] == b'GIF8':
            return ".gif"
        if content[4:12] in (b'ftypavif', b'ftypavis'):
            return ".avif"
        if content[4:12] in (b'ftypheic', b'ftypmif1'):
            return ".heic"
        
        # Content-Type æ£€æµ‹
        if content_type:
            ct = content_type.lower()
            if "png" in ct:
                return ".png"
            if "jpeg" in ct or "jpg" in ct:
                return ".jpeg"
            if "webp" in ct:
                return ".webp"
            if "gif" in ct:
                return ".gif"
        
        # é»˜è®¤ jpeg
        return ".jpeg"

    async def _render_xhs_card(
        self,
        result: XiaohongshuResult,
        image_paths: list[Path],
        cover_path: Path | None,
        is_video: bool,
        request_id: str,
    ) -> Path | None:
        try:
            card_path = self._build_xhs_card_path(result.source_url, request_id)
            image = await asyncio.to_thread(
                self.xhs_renderer.render,
                title=result.title,
                author=result.author,
                text=result.text,
                image_paths=image_paths,
                cover_path=cover_path,
                is_video=is_video,
            )
            await asyncio.to_thread(image.save, card_path, format="PNG")
            return card_path
        except asyncio.CancelledError:
            raise
        except Exception as exc:
            logger.warning("å°çº¢ä¹¦å¡ç‰‡æ¸²æŸ“å¤±è´¥: %s", str(exc))
            return None
    # endregion

    # region å°çº¢ä¹¦å¤„ç†
    async def _process_xhs(
        self, event: AstrMessageEvent, target_link: str, is_from_card: bool = False
    ):
        import time as time_module
        process_start = time_module.perf_counter()
        timing = {}  # è®°å½•å„æ­¥éª¤è€—æ—¶
        
        self._refresh_config()
        if not self.xhs_enabled:
            return
        source_tag = "(æ¥è‡ªå¡ç‰‡)" if is_from_card else ""
        request_id = uuid.uuid4().hex[:8]
        
        # å°è¯•å¢åŠ  aiocqhttp è¶…æ—¶æ—¶é—´ï¼ˆåŸå›¾æ–‡ä»¶è¾ƒå¤§ï¼Œéœ€è¦æ›´é•¿æ—¶é—´ä¸Šä¼ ï¼‰
        self._try_extend_aiocqhttp_timeout(event, getattr(self, 'api_timeout_sec', 600))

        await self._send_reaction_emoji(event, source_tag)

        target_link = (target_link or "").strip()
            
        if not target_link:
            logger.info("âš ï¸ å°çº¢ä¹¦é“¾æ¥ä¸ºç©º%s", source_tag)
            return
        logger.info("ğŸ  å°çº¢ä¹¦è§£æ%s: %s", source_tag, target_link)

        # region è§£æé˜¶æ®µ
        parse_start = time_module.perf_counter()
        retry_count = max(0, int(getattr(self, "retry_count", 3)))
        result: XiaohongshuResult | None = None
        last_error: Exception | None = None

        for attempt in range(retry_count + 1):
            try:
                result = await asyncio.wait_for(
                    self.xhs_extractor.parse(target_link),
                    timeout=XHS_PARSE_TIMEOUT_SEC,
                )
                break
            except asyncio.CancelledError:
                logger.info("â™»ï¸ å°çº¢ä¹¦è§£æä»»åŠ¡å·²ä¸­æ–­%s", source_tag)
                return
            except XiaohongshuParseError as exc:
                last_error = exc
                if attempt < retry_count and self._is_retryable_xhs_exception(exc):
                    wait_time = min(
                        XHS_PARSE_RETRY_MAX_DELAY_SEC,
                        XHS_PARSE_RETRY_BASE_DELAY_SEC * (2 ** attempt),
                    )
                    logger.warning(
                        "âš ï¸ å°çº¢ä¹¦è§£æå¤±è´¥%s: %sï¼Œ%.1fsåé‡è¯• (%d/%d)",
                        source_tag,
                        str(exc),
                        wait_time,
                        attempt + 1,
                        retry_count,
                    )
                    await asyncio.sleep(wait_time)
                    continue
                logger.error("âŒ å°çº¢ä¹¦è§£æå¤±è´¥%s: %s", source_tag, str(exc))
                return
            except Exception as exc:
                last_error = exc
                if attempt < retry_count and self._is_retryable_xhs_exception(exc):
                    wait_time = min(
                        XHS_PARSE_RETRY_MAX_DELAY_SEC,
                        XHS_PARSE_RETRY_BASE_DELAY_SEC * (2 ** attempt),
                    )
                    logger.warning(
                        "âš ï¸ å°çº¢ä¹¦è§£æå¼‚å¸¸%s: %sï¼Œ%.1fsåé‡è¯• (%d/%d)",
                        source_tag,
                        str(exc),
                        wait_time,
                        attempt + 1,
                        retry_count,
                    )
                    await asyncio.sleep(wait_time)
                    continue
                logger.error("âŒ å°çº¢ä¹¦è§£æå¼‚å¸¸%s: %s", source_tag, str(exc))
                return

        if result is None:
            logger.error(
                "âŒ å°çº¢ä¹¦è§£ææœ€ç»ˆå¤±è´¥%s: %s, link=%s, timeout=%.0fs, retries=%d",
                source_tag,
                str(last_error) if last_error else "unknown",
                target_link,
                XHS_PARSE_TIMEOUT_SEC,
                retry_count,
            )
            return

        timing["parse"] = time_module.perf_counter() - parse_start
        # endregion

        logger.debug(
            "ğŸ  å°çº¢ä¹¦è§£æå®Œæˆ%s: è§†é¢‘=%s, å›¾ç‰‡=%s, è§£æè€—æ—¶=%.2fs",
            source_tag,
            "æœ‰" if result.video_url else "æ— ",
            len(result.image_urls),
            timing["parse"],
        )

        title = result.title or "æœªçŸ¥æ ‡é¢˜"
        author = result.author or "æœªçŸ¥ä½œè€…"

        if not result.video_url and not result.image_urls:
            logger.warning("âŒ å°çº¢ä¹¦æœªæ‰¾åˆ°å¯ä¸‹è½½çš„åª’ä½“%s: %s", source_tag, target_link)
            return

        media_components: list[object] = []
        media_paths: list[Path] = []
        image_paths: list[Path] = []
        cover_path: Path | None = None
        failed_images = 0

        # region ä¸‹è½½é˜¶æ®µ
        download_start = time_module.perf_counter()
        
        # è§†é¢‘ç¬”è®°ï¼šä¼˜å…ˆä¸‹è½½è§†é¢‘
        if result.video_url:
            try:
                video_path = await self._download_xhs_video(
                    result.video_url, request_id, referer=result.source_url
                )
                media_paths.append(video_path)
                media_components.append(Video.fromFileSystem(str(video_path.resolve())))
                # ä¸‹è½½å°é¢å›¾
                cover_url = result.cover_url or (result.image_urls[0] if result.image_urls else None)
                if cover_url:
                    try:
                        cover_path = await self._download_xhs_image(
                            cover_url, request_id, referer=result.source_url
                        )
                        media_paths.append(cover_path)
                    except asyncio.CancelledError:
                        raise
                    except Exception as exc:
                        logger.warning("å°çº¢ä¹¦å°é¢ä¸‹è½½å¤±è´¥%s: %s", source_tag, str(exc))
            except asyncio.CancelledError:
                raise
            except SizeLimitExceeded:
                logger.warning("âŒ å°çº¢ä¹¦è§†é¢‘å¤§å°è¶…è¿‡é™åˆ¶%s (%dMB)", source_tag, self.max_video_size_mb)
                return
            except Exception as exc:
                logger.error("âŒ å°çº¢ä¹¦è§†é¢‘ä¸‹è½½å¤±è´¥%s: %s", source_tag, str(exc))
                return
        # å›¾ç‰‡ç¬”è®°ï¼šä¸‹è½½å›¾ç‰‡
        elif result.image_urls:
            image_urls = result.image_urls[: self.xhs_max_media]
            file_ids = result.file_ids[: self.xhs_max_media] if result.file_ids else []
            for i, url in enumerate(image_urls):
                try:
                    # è·å–å¯¹åº”çš„ file_idï¼ˆå¦‚æœæœ‰ï¼‰
                    file_id = file_ids[i] if i < len(file_ids) else None
                    image_path = await self._download_xhs_image(
                        url, request_id, file_id=file_id, referer=result.source_url
                    )
                    image_paths.append(image_path)
                    media_paths.append(image_path)
                    media_components.append(Image.fromFileSystem(str(image_path.resolve())))
                except asyncio.CancelledError:
                    raise
                except Exception as exc:
                    failed_images += 1
                    logger.warning("å°çº¢ä¹¦å›¾ç‰‡ä¸‹è½½å¤±è´¥%s [%d/%d]: %s", source_tag, i + 1, len(image_urls), str(exc))
        
        timing["download"] = time_module.perf_counter() - download_start
        # endregion

        if not media_components:
            logger.debug(
                "XHS æ— åª’ä½“ä¸‹è½½æˆåŠŸ%s: url=%s, å¤±è´¥å›¾ç‰‡=%d, ä¸‹è½½è€—æ—¶=%.2fs",
                source_tag, result.source_url, failed_images, timing["download"]
            )
            return

        # region æ¸²æŸ“é˜¶æ®µ
        render_start = time_module.perf_counter()
        card_path = await self._render_xhs_card(
            result,
            image_paths=image_paths,
            cover_path=cover_path,
            is_video=bool(result.video_url and not image_paths),
            request_id=request_id,
        )
        if card_path:
            media_paths.append(card_path)
            media_components.insert(0, Image.fromFileSystem(str(card_path.resolve())))
        timing["render"] = time_module.perf_counter() - render_start
        # endregion

        # region å‘é€é˜¶æ®µ
        send_start = time_module.perf_counter()
        
        # è®¡ç®—æ€»å¤§å°
        total_size_bytes = await asyncio.to_thread(
            lambda: sum(p.stat().st_size for p in media_paths if p.exists())
        )
        total_size_mb = total_size_bytes / (1024 * 1024)
        
        # åˆ¤æ–­æ˜¯å¦è§¦å‘è§£åˆé˜ˆå€¼
        threshold = getattr(self, 'xhs_auto_unmerge_threshold_mb', 20)
        force_unmerge = False
        if threshold > 0 and total_size_mb > threshold:
            logger.info("XHS åª’ä½“æ€»å¤§å° (%.2fMB) è¶…è¿‡é˜ˆå€¼ (%dMB)ï¼Œå¼ºåˆ¶é€æ¡å‘é€", total_size_mb, threshold)
            force_unmerge = True
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå›¾æ–‡ç¬”è®°ï¼ˆæœ‰å›¾ç‰‡è·¯å¾„ï¼‰
        is_image_post = bool(image_paths)
        # å›¾æ–‡ç¬”è®°å§‹ç»ˆåˆå¹¶è½¬å‘ï¼›è§†é¢‘ç¬”è®°æ ¹æ®é…ç½®å†³å®š
        # force_unmerge ä»…å¯¹å›¾æ–‡ç¬”è®°ç”Ÿæ•ˆï¼ˆé€æ¡å‘é€å›¾ç‰‡ï¼‰
        if is_image_post:
            # å›¾æ–‡ç¬”è®°ï¼šå§‹ç»ˆåˆå¹¶è½¬å‘ï¼ˆé™¤éè§¦å‘è§£åˆé˜ˆå€¼ï¼‰
            should_merge = not force_unmerge
        else:
            # è§†é¢‘ç¬”è®°ï¼šæ ¹æ®é…ç½®å†³å®š
            should_merge = self.xhs_merge_send

        if should_merge:
            nodes = Nodes([])
            sender_uin = self._get_merge_sender_uin(event)
            for component in media_components:
                nodes.nodes.append(Node(uin=sender_uin, content=[component]))
            await event.send(MessageChain([nodes]))
        else:
            if is_image_post:
                # å›¾æ–‡ç¬”è®°é€æ¡å‘é€ï¼ˆè§¦å‘è§£åˆé˜ˆå€¼æ—¶ï¼‰
                for i, component in enumerate(media_components):
                    await event.send(MessageChain([component]))
                    if i < len(media_components) - 1:
                        await asyncio.sleep(2.0)
            else:
                # è§†é¢‘ç¬”è®°ä¸åˆå¹¶å‘é€ï¼šåªå‘é€è§†é¢‘ï¼ˆä¸å«å¡ç‰‡ï¼‰
                # æ‰¾åˆ°è§†é¢‘ç»„ä»¶ï¼ˆç¬¬ä¸€ä¸ªéå¡ç‰‡çš„ç»„ä»¶ï¼‰
                for component in media_components:
                    if isinstance(component, Video):
                        await event.send(MessageChain([component]))
                        break

        timing["send"] = time_module.perf_counter() - send_start
        # endregion

        # è¾“å‡ºå®Œæ•´è€—æ—¶æ—¥å¿—
        total_elapsed = time_module.perf_counter() - process_start
        logger.info(
            "ğŸ  XHS å¤„ç†å®Œæˆ%s: æ ‡é¢˜=%s, åª’ä½“=%d, å¤±è´¥=%d | è€—æ—¶: è§£æ=%.2fs, ä¸‹è½½=%.2fs, æ¸²æŸ“=%.2fs, å‘é€=%.2fs, æ€»è®¡=%.2fs",
            source_tag,
            title[:20],
            len(media_components),
            failed_images,
            timing.get("parse", 0),
            timing.get("download", 0),
            timing.get("render", 0),
            timing.get("send", 0),
            total_elapsed,
        )

        # å‘é€å®Œæˆåç«‹å³æ¸…ç†æ–‡ä»¶ï¼ˆDirect Send Patternï¼šæ­¤æ—¶æ–‡ä»¶å·²è¢«è¯»å–ï¼‰
        if media_paths:
            await self.cleanup_files(media_paths, [])
    # endregion

    # region äº‹ä»¶å¤„ç†å™¨
    # äº‹ä»¶è¿‡æ»¤å™¨ç”± main.py æ³¨å†Œï¼Œç¡®ä¿ç»‘å®šæ’ä»¶å®ä¾‹ã€‚
    async def handle_xhs(self, event: AstrMessageEvent):
        if not self.xhs_enabled:
            return
        if self._is_self_message(event):
            return
        if await self._is_bot_muted(event):
            return
        event.should_call_llm(True)
        links = extract_xhs_links(event.message_str)
        logger.info("ğŸ  å°çº¢ä¹¦åŒ¹é…é“¾æ¥: %s", links)
        if not links:
            return
        try:
            await self._process_xhs(event, links[0], is_from_card=False)
        except asyncio.CancelledError:
            logger.info("â™»ï¸ å°çº¢ä¹¦è§£æä»»åŠ¡å·²ä¸­æ–­")
            return
    # endregion


# endregion
