# region å¯¼å…¥
import asyncio
import re
from pathlib import Path
from urllib.parse import urlparse

import aiohttp
from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent
from astrbot.api.message_components import Image, Node, Nodes, Video

from ..common import XHS_VIDEO_PATH, XHS_IMAGE_PATH, XHS_CARD_PATH, SizeLimitExceeded
from . import (
    XHS_HEADERS,
    XHS_MESSAGE_PATTERN,
    XiaohongshuParseError,
    XiaohongshuResult,
    extract_xhs_links,
    load_xhs_cookies,
)
# endregion


# region å°çº¢ä¹¦æ··å…¥
class XiaohongshuMixin:
    # region è·¯å¾„ä¸å€™é€‰æ„å»º
    def _build_xhs_path(self, url: str, is_video: bool) -> Path:
        suffix = ".mp4" if is_video else self._guess_media_suffix(url, ".jpg")
        base_dir = XHS_VIDEO_PATH if is_video else XHS_IMAGE_PATH
        return base_dir / f"{self._hash_url(url)}{suffix}"

    def _build_xhs_card_path(self, source_url: str) -> Path:
        return XHS_CARD_PATH / f"{self._hash_url(source_url)}_card.png"

    @staticmethod
    def _force_https(url: str) -> str:
        try:
            parsed = urlparse(url)
        except Exception:
            return url
        if parsed.scheme in ("", "http"):
            return parsed._replace(scheme="https").geturl()
        return url
    # endregion

    # region ä¸‹è½½ä¸æ¸²æŸ“
    @staticmethod
    def _xhs_download_headers(referer: str | None) -> dict[str, str]:
        headers = dict(XHS_HEADERS)
        if referer:
            headers["Referer"] = referer
        headers["Origin"] = "https://www.xiaohongshu.com"
        headers["Accept"] = "image/avif,image/webp,image/apng,image/*,*/*;q=0.8"
        headers["Accept-Language"] = "zh-CN,zh;q=0.9"
        return headers

    def _get_xhs_cookies(self) -> dict[str, str]:
        """è·å–å°çº¢ä¹¦ cookiesï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨ï¼‰"""
        # æ£€æŸ¥é…ç½®æ˜¯å¦å¯ç”¨ cookies
        if not getattr(self, 'xhs_use_cookies', False):
            return {}
        if not hasattr(self, "_xhs_cookies_cache"):
            self._xhs_cookies_cache = load_xhs_cookies()
        return self._xhs_cookies_cache

    def _try_extend_aiocqhttp_timeout(self, event: AstrMessageEvent, timeout_sec: float) -> None:
        """å°è¯•å»¶é•¿ aiocqhttp API è¶…æ—¶æ—¶é—´ï¼ˆç”¨äºå‘é€å¤§å›¾ï¼‰
        
        è·¯å¾„: event.bot._api._wsr_api._timeout_sec
        """
        try:
            bot = getattr(event, 'bot', None)
            if not bot:
                return
            api = getattr(bot, '_api', None)
            if not api:
                return
            wsr_api = getattr(api, '_wsr_api', None)
            if not wsr_api:
                return
            current_timeout = getattr(wsr_api, '_timeout_sec', 180)
            if current_timeout < timeout_sec:
                wsr_api._timeout_sec = timeout_sec
                logger.debug("aiocqhttp API è¶…æ—¶å·²ä» %.0fs å»¶é•¿åˆ° %.0fs", current_timeout, timeout_sec)
        except Exception as e:
            logger.debug("æ— æ³•ä¿®æ”¹ aiocqhttp è¶…æ—¶: %s", str(e))

    async def _download_xhs_video(self, url: str, referer: str | None = None) -> Path:
        max_bytes = self.max_video_size_mb * 1024 * 1024 if self.max_video_size_mb > 0 else None
        cookies = self._get_xhs_cookies()
        size_mb = await self._estimate_total_size_mb(
            url, None, headers=self._xhs_download_headers(referer), cookies=cookies
        )
        logger.info(
            "ğŸ“¹ ä¼°ç®—å°çº¢ä¹¦è§†é¢‘å¤§å°: %s MB",
            f"{size_mb:.2f}" if size_mb is not None else "æœªçŸ¥",
        )
        if size_mb is not None and max_bytes and size_mb * 1024 * 1024 > max_bytes:
            raise SizeLimitExceeded("è¶…è¿‡å¤§å°é™åˆ¶")
        output_path = self._build_xhs_path(url, is_video=True)
        await self._download_stream(
            url,
            output_path,
            cookies=cookies,
            max_bytes=max_bytes,
            headers=self._xhs_download_headers(referer),
            retries=3,
        )
        return output_path

    async def _download_xhs_image(
        self, 
        url: str, 
        file_id: str | None = None,
        referer: str | None = None
    ) -> Path:
        """ä¸‹è½½å›¾ç‰‡ - ä¸‰çº§å›é€€ç­–ç•¥
        
        1. å¦‚æœå¼€å¯åŸå›¾ä¸‹è½½ï¼šå°è¯• PNG åŸå›¾ (ci.xiaohongshu.com)
        2. å¦‚æœ PNG å¤±è´¥ï¼šå°è¯• JPEG åŸå›¾
        3. å¦‚æœéƒ½å¤±è´¥ï¼šå›é€€åˆ°å¤š CDN å…œåº•ç­–ç•¥
        """
        import time as time_module
        start_time = time_module.perf_counter()
        
        output_path = self._build_xhs_path(url, is_video=False)
        cookies = self._get_xhs_cookies()
        
        # æå– image token (å‚è€ƒ XHS-Downloader)
        token = self._extract_image_token(url)
        logger.debug("XHS å›¾ç‰‡ä¸‹è½½å¼€å§‹: url=%s, file_id=%s, token=%s", url[:80], file_id, token)
        
        # region åŸå›¾ä¸‹è½½å°è¯•
        if getattr(self, 'xhs_download_original', True) and token:
            original_start = time_module.perf_counter()
            
            # æ„å»ºåŸå›¾ URL å€™é€‰åˆ—è¡¨
            # ç­–ç•¥ï¼šä¼˜å…ˆç›´æ¥ CDN é“¾æ¥ï¼ˆæ— å‹ç¼©åŸå›¾ï¼‰ï¼Œå†å°è¯• imageView2 æ ¼å¼è½¬æ¢
            original_candidates = []
            
            # 1. ç›´æ¥ CDN é“¾æ¥ - è·å–æœªå‹ç¼©çš„åŸå§‹æ–‡ä»¶ï¼ˆXHS-Downloader çš„ auto æ¨¡å¼ï¼‰
            cdn_domains = [
                "sns-img-bd.xhscdn.com",  # XHS-Downloader é»˜è®¤ä½¿ç”¨
                "sns-img-qc.xhscdn.com",
                "sns-img-hw.xhscdn.com",
            ]
            for domain in cdn_domains:
                original_candidates.append({
                    "url": f"https://{domain}/{token}",
                    "desc": f"CDN-{domain.split('-')[2].split('.')[0]}-åŸå§‹",
                    "format": None,  # ä¿æŒåŸå§‹æ ¼å¼
                })
            
            # 2. imageView2 æ ¼å¼è½¬æ¢ - ä½œä¸ºå¤‡é€‰ï¼ˆå¯èƒ½ä¼šå‹ç¼©ï¼Œä½†ä¿è¯æ ¼å¼ï¼‰
            for format_name in ["png", "jpeg"]:
                original_candidates.append({
                    "url": f"https://ci.xiaohongshu.com/{token}?imageView2/format/{format_name}",
                    "desc": f"CI-{format_name.upper()}",
                    "format": format_name,
                })
            
            for cand in original_candidates:
                cand_url = cand["url"]
                desc = cand["desc"]
                format_name = cand["format"]
                attempt_start = time_module.perf_counter()
                
                try:
                    # ä½¿ç”¨éå¸¸é•¿çš„è¶…æ—¶æ—¶é—´ï¼ˆç”¨æˆ·è¯´å¸¦å®½å°ï¼‰
                    timeout = aiohttp.ClientTimeout(total=600, connect=60)
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Referer": "https://www.xiaohongshu.com/",
                    }
                    
                    async with aiohttp.ClientSession(
                        headers=headers,
                        cookies=cookies if cookies else None,
                        timeout=timeout
                    ) as session:
                        async with session.get(cand_url) as resp:
                            attempt_elapsed = time_module.perf_counter() - attempt_start
                            
                            if resp.status == 200:
                                content = await resp.read()
                                content_len = len(content)
                                
                                # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆè‡³å°‘ 10KB æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆå›¾ç‰‡ï¼‰
                                if content_len >= 10 * 1024:
                                    # ç¡®å®šè¾“å‡ºæ–‡ä»¶åç¼€
                                    if format_name:
                                        actual_suffix = f".{format_name}"
                                    else:
                                        # ä»å“åº”å¤´æˆ–æ–‡ä»¶ç­¾åæ¨æ–­æ ¼å¼
                                        actual_suffix = self._detect_image_suffix(content, resp.headers.get("Content-Type"))
                                    
                                    final_output = output_path.with_suffix(actual_suffix)
                                    temp_path = final_output.with_suffix(final_output.suffix + ".part")
                                    with open(temp_path, "wb") as f:
                                        f.write(content)
                                    if temp_path.exists():
                                        temp_path.replace(final_output)
                                    
                                    total_elapsed = time_module.perf_counter() - start_time
                                    logger.info(
                                        "XHS åŸå›¾ä¸‹è½½æˆåŠŸ (%s): size=%.1fMB, è¯·æ±‚è€—æ—¶=%.2fs, æ€»è€—æ—¶=%.2fs",
                                        desc, content_len / 1024 / 1024, attempt_elapsed, total_elapsed
                                    )
                                    return final_output
                                else:
                                    logger.debug(
                                        "XHS åŸå›¾å“åº”è¿‡å° (%s): size=%d bytes, è€—æ—¶=%.2fs",
                                        desc, content_len, attempt_elapsed
                                    )
                            else:
                                logger.debug(
                                    "XHS åŸå›¾è¯·æ±‚å¤±è´¥ (%s): HTTP %d, è€—æ—¶=%.2fs",
                                    desc, resp.status, attempt_elapsed
                                )
                except asyncio.CancelledError:
                    raise
                except Exception as e:
                    attempt_elapsed = time_module.perf_counter() - attempt_start
                    logger.debug(
                        "XHS åŸå›¾ä¸‹è½½å¼‚å¸¸ (%s): %s, è€—æ—¶=%.2fs",
                        desc, str(e)[:50], attempt_elapsed
                    )
            
            original_elapsed = time_module.perf_counter() - original_start
            logger.debug("XHS åŸå›¾ä¸‹è½½å…¨éƒ¨å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šç­–ç•¥ï¼ŒåŸå›¾å°è¯•è€—æ—¶=%.2fs", original_elapsed)
        # endregion
        
        # region CDN å…œåº•ç­–ç•¥
        fallback_start = time_module.perf_counter()
        
        # åŸºç¡€ Headers
        base_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        }
        
        # æ„å»ºé˜¶æ¢¯å€™é€‰åˆ—è¡¨
        candidates = []
        
        # 1. åŸå§‹ URL (å¸¦ç­¾å)
        raw_url = url.replace("http://", "https://", 1) if url.startswith("http://") else url
        candidates.append({"url": raw_url, "use_cookies": False, "desc": "Raw-NoCookie"})
        candidates.append({"url": raw_url, "use_cookies": True, "desc": "Raw-WithCookie"})

        effective_id = file_id or token
        if effective_id:
            # 2. æ— ç­¾åé€šç”¨ CDN (å…œåº•æ–¹æ¡ˆ)
            domains = [
                "sns-img-bd.xhscdn.com",
                "sns-img-qc.xhscdn.com", 
                "sns-img-hw.xhscdn.com", 
                "sns-webpic-qc.xhscdn.com",
            ]
            for domain in domains:
                for path_prefix in ["", "spectrum/"]:
                    path = f"{path_prefix}{effective_id}"
                    candidates.append({"url": f"https://{domain}/{path}", "use_cookies": False, "desc": f"CDN-{domain.split('.')[0]}"})
        
        errors = []
        for cand in candidates:
            cand_url = cand["url"]
            use_cookies_flag = cand["use_cookies"]
            desc = cand["desc"]
            attempt_start = time_module.perf_counter()
            
            # ä¸¤ç§ header å˜ä½“
            header_variants = [
                {**base_headers, "Referer": "https://www.xiaohongshu.com/"},
                base_headers.copy(),
            ]
            
            for hv in header_variants:
                try:
                    # è¶…é•¿è¶…æ—¶
                    timeout = aiohttp.ClientTimeout(total=300, connect=30)
                    async with aiohttp.ClientSession(
                        headers=hv, 
                        cookies=cookies if use_cookies_flag else None,
                        timeout=timeout
                    ) as session:
                        async with session.get(cand_url) as resp:
                            if resp.status == 200:
                                content = await resp.read()
                                if len(content) >= 1024:
                                    temp_path = output_path.with_suffix(output_path.suffix + ".part")
                                    with open(temp_path, "wb") as f:
                                        f.write(content)
                                    if temp_path.exists():
                                        temp_path.replace(output_path)
                                    
                                    attempt_elapsed = time_module.perf_counter() - attempt_start
                                    total_elapsed = time_module.perf_counter() - start_time
                                    logger.info(
                                        "XHS CDN å›¾ç‰‡ä¸‹è½½æˆåŠŸ (%s): size=%.1fKB, è¯·æ±‚è€—æ—¶=%.2fs, æ€»è€—æ—¶=%.2fs",
                                        desc, len(content) / 1024, attempt_elapsed, total_elapsed
                                    )
                                    return output_path
                            
                            errors.append(f"{desc}: HTTP {resp.status}")
                except asyncio.CancelledError:
                    raise
                except Exception as e:
                    errors.append(f"{desc}: {str(e)[:20]}")
                    continue
        # endregion
        
        # å…¨éƒ¨å¤±è´¥
        total_elapsed = time_module.perf_counter() - start_time
        error_summary = " | ".join(errors[:5])  # åªå–å‰5ä¸ªé”™è¯¯
        logger.error("XHS å›¾ç‰‡ä¸‹è½½å…¨çº¿å¤±è´¥: æ€»è€—æ—¶=%.2fs, é”™è¯¯=%s", total_elapsed, error_summary)
        raise RuntimeError(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {error_summary}")
    
    @staticmethod
    def _extract_image_token(url: str) -> str | None:
        """ä» URL ä¸­æå– image tokenï¼ˆå‚è€ƒ XHS-Downloaderï¼‰"""
        if not url:
            return None
        try:
            # æ ¼å¼: https://xxx.xhscdn.com/spectrum/1040g0k... æˆ–ç±»ä¼¼
            # æå–è·¯å¾„ç¬¬5ä¸ª/ä¹‹åçš„éƒ¨åˆ†ï¼Œå»æ‰!åç¼€
            parts = url.split("/")
            if len(parts) >= 6:
                token = "/".join(parts[5:]).split("!")[0].split("?")[0]
                if token and len(token) > 10:
                    return token
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥å–æœ€åä¸€æ®µ
            last_part = url.split("/")[-1].split("!")[0].split("?")[0]
            if last_part and len(last_part) > 10:
                return last_part
        except Exception:
            pass
        return None
    
    @staticmethod
    def _detect_image_suffix(content: bytes, content_type: str | None) -> str:
        """ä»æ–‡ä»¶ç­¾åæˆ– Content-Type æ£€æµ‹å›¾ç‰‡æ ¼å¼"""
        # æ–‡ä»¶ç­¾åæ£€æµ‹ï¼ˆé­”æ•°ï¼‰
        if content[:8] == b'\x89PNG\r\n\x1a\n':
            return ".png"
        if content[:3] == b'\xff\xd8\xff':
            return ".jpeg"
        if content[:4] == b'RIFF' and content[8:12] == b'WEBP':
            return ".webp"
        if content[:4] == b'GIF8':
            return ".gif"
        if content[4:12] in (b'ftypavif', b'ftypavis'):
            return ".avif"
        if content[4:12] in (b'ftypheic', b'ftypmif1'):
            return ".heic"
        
        # Content-Type æ£€æµ‹
        if content_type:
            ct = content_type.lower()
            if "png" in ct:
                return ".png"
            if "jpeg" in ct or "jpg" in ct:
                return ".jpeg"
            if "webp" in ct:
                return ".webp"
            if "gif" in ct:
                return ".gif"
        
        # é»˜è®¤ jpeg
        return ".jpeg"

    async def _render_xhs_card(
        self,
        result: XiaohongshuResult,
        image_paths: list[Path],
        cover_path: Path | None,
        is_video: bool,
    ) -> Path | None:
        try:
            card_path = self._build_xhs_card_path(result.source_url)
            image = await asyncio.to_thread(
                self.xhs_renderer.render,
                title=result.title,
                author=result.author,
                text=result.text,
                image_paths=image_paths,
                cover_path=cover_path,
                is_video=is_video,
            )
            await asyncio.to_thread(image.save, card_path, format="PNG")
            return card_path
        except asyncio.CancelledError:
            raise
        except Exception as exc:
            logger.warning("å°çº¢ä¹¦å¡ç‰‡æ¸²æŸ“å¤±è´¥: %s", str(exc))
            return None
    # endregion

    # region å°çº¢ä¹¦å¤„ç†
    async def _process_xhs(
        self, event: AstrMessageEvent, target_link: str, is_from_card: bool = False
    ):
        import time as time_module
        process_start = time_module.perf_counter()
        timing = {}  # è®°å½•å„æ­¥éª¤è€—æ—¶
        
        self._refresh_config()
        if not self.xhs_enabled:
            return
        source_tag = "(æ¥è‡ªå¡ç‰‡)" if is_from_card else ""
        
        # å°è¯•å¢åŠ  aiocqhttp è¶…æ—¶æ—¶é—´ï¼ˆåŸå›¾æ–‡ä»¶è¾ƒå¤§ï¼Œéœ€è¦æ›´é•¿æ—¶é—´ä¸Šä¼ ï¼‰
        self._try_extend_aiocqhttp_timeout(event, getattr(self, 'api_timeout_sec', 600))

        await self._send_reaction_emoji(event, source_tag)

        target_link = (target_link or "").strip()
        
        # åˆæ­¥å»é‡
        if target_link and not self._check_and_record_url(target_link):
            return
            
        if not target_link:
            logger.info("âš ï¸ å°çº¢ä¹¦é“¾æ¥ä¸ºç©º%s", source_tag)
            return
        logger.info("ğŸ  å°çº¢ä¹¦è§£æ%s: %s", source_tag, target_link)

        # region è§£æé˜¶æ®µ
        parse_start = time_module.perf_counter()
        try:
            result: XiaohongshuResult = await asyncio.wait_for(
                self.xhs_extractor.parse(target_link),
                timeout=30.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
        except asyncio.CancelledError:
            logger.info("â™»ï¸ å°çº¢ä¹¦è§£æä»»åŠ¡å·²ä¸­æ–­%s", source_tag)
            return
        except asyncio.TimeoutError:
            logger.error("âŒ å°çº¢ä¹¦è§£æè¶…æ—¶%s", source_tag)
            # ä¸å‘ç”¨æˆ·å‘é€é”™è¯¯ä¿¡æ¯
            return
        except XiaohongshuParseError as exc:
            logger.error("âŒ å°çº¢ä¹¦è§£æå¤±è´¥%s: %s", source_tag, str(exc))
            return
        except Exception as exc:
            logger.error("âŒ å°çº¢ä¹¦è§£æå¼‚å¸¸%s: %s", source_tag, str(exc))
            return
        timing["parse"] = time_module.perf_counter() - parse_start
        # endregion

        # æ ¹æ® note_id å»é‡
        if result.note_id and not self._check_and_record_url(f"xhs:{result.note_id}"):
            return

        logger.debug(
            "ğŸ  å°çº¢ä¹¦è§£æå®Œæˆ%s: è§†é¢‘=%s, å›¾ç‰‡=%s, è§£æè€—æ—¶=%.2fs",
            source_tag,
            "æœ‰" if result.video_url else "æ— ",
            len(result.image_urls),
            timing["parse"],
        )

        title = result.title or "æœªçŸ¥æ ‡é¢˜"
        author = result.author or "æœªçŸ¥ä½œè€…"

        if not result.video_url and not result.image_urls:
            logger.warning("âŒ å°çº¢ä¹¦æœªæ‰¾åˆ°å¯ä¸‹è½½çš„åª’ä½“%s: %s", source_tag, target_link)
            return

        media_components: list[object] = []
        media_paths: list[Path] = []
        image_paths: list[Path] = []
        cover_path: Path | None = None
        failed_images = 0

        # region ä¸‹è½½é˜¶æ®µ
        download_start = time_module.perf_counter()
        
        # è§†é¢‘ç¬”è®°ï¼šä¼˜å…ˆä¸‹è½½è§†é¢‘
        if result.video_url:
            try:
                video_path = await self._download_xhs_video(
                    result.video_url, referer=result.source_url
                )
                media_paths.append(video_path)
                media_components.append(Video.fromFileSystem(str(video_path.resolve())))
                # ä¸‹è½½å°é¢å›¾
                cover_url = result.cover_url or (result.image_urls[0] if result.image_urls else None)
                if cover_url:
                    try:
                        cover_path = await self._download_xhs_image(
                            cover_url, referer=result.source_url
                        )
                        media_paths.append(cover_path)
                    except asyncio.CancelledError:
                        raise
                    except Exception as exc:
                        logger.warning("å°çº¢ä¹¦å°é¢ä¸‹è½½å¤±è´¥%s: %s", source_tag, str(exc))
            except asyncio.CancelledError:
                raise
            except SizeLimitExceeded:
                logger.warning("âŒ å°çº¢ä¹¦è§†é¢‘å¤§å°è¶…è¿‡é™åˆ¶%s (%dMB)", source_tag, self.max_video_size_mb)
                return
            except Exception as exc:
                logger.error("âŒ å°çº¢ä¹¦è§†é¢‘ä¸‹è½½å¤±è´¥%s: %s", source_tag, str(exc))
                return
        # å›¾ç‰‡ç¬”è®°ï¼šä¸‹è½½å›¾ç‰‡
        elif result.image_urls:
            image_urls = result.image_urls[: self.xhs_max_media]
            file_ids = result.file_ids[: self.xhs_max_media] if result.file_ids else []
            for i, url in enumerate(image_urls):
                try:
                    # è·å–å¯¹åº”çš„ file_idï¼ˆå¦‚æœæœ‰ï¼‰
                    file_id = file_ids[i] if i < len(file_ids) else None
                    image_path = await self._download_xhs_image(
                        url, file_id=file_id, referer=result.source_url
                    )
                    image_paths.append(image_path)
                    media_paths.append(image_path)
                    media_components.append(Image.fromFileSystem(str(image_path.resolve())))
                except asyncio.CancelledError:
                    raise
                except Exception as exc:
                    failed_images += 1
                    logger.warning("å°çº¢ä¹¦å›¾ç‰‡ä¸‹è½½å¤±è´¥%s [%d/%d]: %s", source_tag, i + 1, len(image_urls), str(exc))
        
        timing["download"] = time_module.perf_counter() - download_start
        # endregion

        if not media_components:
            logger.debug(
                "XHS æ— åª’ä½“ä¸‹è½½æˆåŠŸ%s: url=%s, å¤±è´¥å›¾ç‰‡=%d, ä¸‹è½½è€—æ—¶=%.2fs",
                source_tag, result.source_url, failed_images, timing["download"]
            )
            return

        # region æ¸²æŸ“é˜¶æ®µ
        render_start = time_module.perf_counter()
        card_path = await self._render_xhs_card(
            result,
            image_paths=image_paths,
            cover_path=cover_path,
            is_video=bool(result.video_url and not image_paths),
        )
        if card_path:
            media_paths.append(card_path)
            media_components.insert(0, Image.fromFileSystem(str(card_path.resolve())))
        timing["render"] = time_module.perf_counter() - render_start
        # endregion

        # region å‘é€é˜¶æ®µ
        send_start = time_module.perf_counter()
        
        # è®¡ç®—æ€»å¤§å°
        total_size_bytes = sum(p.stat().st_size for p in media_paths if p.exists())
        total_size_mb = total_size_bytes / (1024 * 1024)
        
        # åˆ¤æ–­æ˜¯å¦è§¦å‘è§£åˆé˜ˆå€¼
        threshold = getattr(self, 'xhs_auto_unmerge_threshold_mb', 20)
        force_unmerge = False
        if threshold > 0 and total_size_mb > threshold:
            logger.info("XHS åª’ä½“æ€»å¤§å° (%.2fMB) è¶…è¿‡é˜ˆå€¼ (%dMB)ï¼Œå¼ºåˆ¶é€æ¡å‘é€", total_size_mb, threshold)
            force_unmerge = True
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå›¾æ–‡ç¬”è®°ï¼ˆæœ‰å›¾ç‰‡è·¯å¾„ï¼‰
        is_image_post = bool(image_paths)
        # å›¾æ–‡ç¬”è®°å§‹ç»ˆåˆå¹¶è½¬å‘ï¼›è§†é¢‘ç¬”è®°æ ¹æ®é…ç½®å†³å®š
        # force_unmerge ä»…å¯¹å›¾æ–‡ç¬”è®°ç”Ÿæ•ˆï¼ˆé€æ¡å‘é€å›¾ç‰‡ï¼‰
        if is_image_post:
            # å›¾æ–‡ç¬”è®°ï¼šå§‹ç»ˆåˆå¹¶è½¬å‘ï¼ˆé™¤éè§¦å‘è§£åˆé˜ˆå€¼ï¼‰
            should_merge = not force_unmerge
        else:
            # è§†é¢‘ç¬”è®°ï¼šæ ¹æ®é…ç½®å†³å®š
            should_merge = self.xhs_merge_send

        if should_merge:
            nodes = Nodes([])
            self_id = str(event.get_self_id())
            for component in media_components:
                nodes.nodes.append(Node(uin=self_id, name="MyParser", content=[component]))
            yield event.chain_result([nodes])
        else:
            if is_image_post:
                # å›¾æ–‡ç¬”è®°é€æ¡å‘é€ï¼ˆè§¦å‘è§£åˆé˜ˆå€¼æ—¶ï¼‰
                for i, component in enumerate(media_components):
                    yield event.chain_result([component])
                    if i < len(media_components) - 1:
                        await asyncio.sleep(2.0)
            else:
                # è§†é¢‘ç¬”è®°ä¸åˆå¹¶å‘é€ï¼šåªå‘é€è§†é¢‘ï¼ˆä¸å«å¡ç‰‡ï¼‰
                # æ‰¾åˆ°è§†é¢‘ç»„ä»¶ï¼ˆç¬¬ä¸€ä¸ªéå¡ç‰‡çš„ç»„ä»¶ï¼‰
                for component in media_components:
                    if isinstance(component, Video):
                        yield event.chain_result([component])
                        break

        timing["send"] = time_module.perf_counter() - send_start
        # endregion

        # è¾“å‡ºå®Œæ•´è€—æ—¶æ—¥å¿—
        total_elapsed = time_module.perf_counter() - process_start
        logger.info(
            "ğŸ  XHS å¤„ç†å®Œæˆ%s: æ ‡é¢˜=%s, åª’ä½“=%d, å¤±è´¥=%d | è€—æ—¶: è§£æ=%.2fs, ä¸‹è½½=%.2fs, æ¸²æŸ“=%.2fs, å‘é€=%.2fs, æ€»è®¡=%.2fs",
            source_tag,
            title[:20],
            len(media_components),
            failed_images,
            timing.get("parse", 0),
            timing.get("download", 0),
            timing.get("render", 0),
            timing.get("send", 0),
            total_elapsed,
        )

        if media_paths:
            asyncio.create_task(self.cleanup_files(media_paths, []))
    # endregion

    # region äº‹ä»¶å¤„ç†å™¨
    # äº‹ä»¶è¿‡æ»¤å™¨ç”± main.py æ³¨å†Œï¼Œç¡®ä¿ç»‘å®šæ’ä»¶å®ä¾‹ã€‚
    async def handle_xhs(self, event: AstrMessageEvent):
        if not self.xhs_enabled:
            return
        if self._is_self_message(event):
            return
        if await self._is_bot_muted(event):
            return
        self._register_parse_task("xhs", event)
        event.should_call_llm(True)
        links = extract_xhs_links(event.message_str)
        logger.info("ğŸ  å°çº¢ä¹¦åŒ¹é…é“¾æ¥: %s", links)
        if not links:
            return
        try:
            async for result in self._process_xhs(event, links[0], is_from_card=False):
                yield result
        except asyncio.CancelledError:
            logger.info("â™»ï¸ å°çº¢ä¹¦è§£æä»»åŠ¡å·²ä¸­æ–­")
            return
    # endregion


# endregion
