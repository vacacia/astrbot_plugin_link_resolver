"""å°çº¢ä¹¦å†…å®¹æå–å™¨ - åŸºäº astrbot_plugin_parser å‚è€ƒå®ç°é‡å†™"""

import json
import re
from dataclasses import dataclass
from http import cookiejar
from pathlib import Path
from typing import Any

import aiohttp
from astrbot.api import logger

# Cookie æ–‡ä»¶è·¯å¾„ï¼ˆä» common æ¨¡å—å¯¼å…¥ï¼‰
from ..common import get_xhs_cookies_file


# region å¸¸é‡
XHS_SHORT_LINK_PATTERN = r"(?:https?://)?(?:www\.)?xhslink\.com/[A-Za-z0-9._?%&+=/#@-]+"
XHS_MESSAGE_PATTERN = (
    r"(?s).*(?:"
    + XHS_SHORT_LINK_PATTERN
    + r"|(?:https?://)?(?:www\.)?xiaohongshu\.com/(?:explore|discovery/item)/[0-9a-zA-Z]+)"
)

# é€šç”¨ headers
_COMMON_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/55.0.2883.87 UBrowser/6.2.4098.3 Safari/537.36"
    )
}

# headers (ç”¨äºçŸ­é“¾æ¥é‡å®šå‘)
XHS_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) "
        "Version/16.6 Mobile/15E148 Safari/604.1 Edg/132.0.0.0"
    ),
    "origin": "https://www.xiaohongshu.com",
    "x-requested-with": "XMLHttpRequest",
    "sec-fetch-site": "same-origin",
    "sec-fetch-mode": "cors",
    "sec-fetch-dest": "empty",
}

# explore é¡µé¢ headers
_EXPLORE_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/55.0.2883.87 UBrowser/6.2.4098.3 Safari/537.36"
    ),
    "accept": (
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,"
        "image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
    ),
}



_SHORT_RE = re.compile(XHS_SHORT_LINK_PATTERN, re.IGNORECASE)
_NOTE_ID_RE = re.compile(r"/(?:explore|discovery/item)/(?P<id>[0-9a-zA-Z]+)", re.IGNORECASE)
_LONG_RE = re.compile(
    r"(?:https?://)?(?:www\.)?xiaohongshu\.com/(?:explore|discovery/item)/[0-9a-zA-Z]+[A-Za-z0-9._%?&+=/#@-]*",
    re.IGNORECASE
)
# endregion


def extract_xhs_links(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰å°çº¢ä¹¦é“¾æ¥"""
    links = []
    # çŸ­é“¾æ¥
    for match in _SHORT_RE.finditer(text):
        url = match.group(0)
        if not url.startswith("http"):
            url = "https://" + url
        links.append(url)
    # é•¿é“¾æ¥
    for match in _LONG_RE.finditer(text):
        url = match.group(0)
        if not url.startswith("http"):
            url = "https://" + url
        if url not in links:
            links.append(url)
    return links


# region æ•°æ®ç±»
@dataclass(slots=True)
class XiaohongshuResult:
    title: str | None
    author: str | None
    text: str | None
    image_urls: list[str]
    file_ids: list[str]  # ç”¨äºå°è¯•ä¸‹è½½åŸå›¾
    video_url: str | None
    cover_url: str | None
    source_url: str
    note_id: str | None = None


class XiaohongshuParseError(RuntimeError):
    pass
# endregion


def load_xhs_cookies() -> dict[str, str]:
    """åŠ è½½å°çº¢ä¹¦ cookiesï¼ˆæ”¯æŒ JSON æ ¼å¼æˆ– Netscape cookies.txt æ ¼å¼ï¼‰"""
    cookies_file = get_xhs_cookies_file()
    if not cookies_file.exists():
        return {}
    try:
        raw = cookies_file.read_text(encoding="utf-8").strip()
        if not raw:
            return {}
        # å°è¯• JSON æ ¼å¼
        if raw.lstrip().startswith("{"):
            data = json.loads(raw)
            if isinstance(data, dict):
                cookies = {str(k): str(v) for k, v in data.items()}
                logger.info("ğŸª å°çº¢ä¹¦ cookies åŠ è½½æˆåŠŸ (JSON): %d ä¸ª", len(cookies))
                return cookies
    except Exception:
        pass

    # å°è¯• Netscape cookies.txt æ ¼å¼
    try:
        jar = cookiejar.MozillaCookieJar()
        jar.load(str(get_xhs_cookies_file()), ignore_discard=True, ignore_expires=True)
        cookies = {cookie.name: cookie.value for cookie in jar}
        logger.info("ğŸª å°çº¢ä¹¦ cookies åŠ è½½æˆåŠŸ (Netscape): %d ä¸ª", len(cookies))
        return cookies
    except Exception as exc:
        logger.warning("ğŸª å°çº¢ä¹¦ cookies åŠ è½½å¤±è´¥: %s", str(exc))
        return {}


class XiaohongshuExtractor:
    """å°çº¢ä¹¦å†…å®¹æå–å™¨"""

    def __init__(self, timeout: float = 15.0, cookies: dict[str, str] | None = None):
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        self.cookies = cookies or load_xhs_cookies()

    async def parse(self, text_or_url: str) -> XiaohongshuResult:
        """è§£æå°çº¢ä¹¦é“¾æ¥"""
        url = text_or_url.strip()
        if not url.startswith("http"):
            url = "https://" + url

        # çŸ­é“¾æ¥éœ€è¦å…ˆè·å–é‡å®šå‘ç›®æ ‡
        if _SHORT_RE.search(url):
            url = await self._get_redirect_url(url)
            logger.debug("XHS short url resolved: %s", url)

        # æå– note_id
        match = _NOTE_ID_RE.search(url)
        if not match:
            raise XiaohongshuParseError(f"æ— æ³•ä»URLæå–ç¬”è®°ID: {url}")
        note_id = match.group("id")
        logger.debug("XHS note id: %s", note_id)

        # æ„å»º explore URLï¼ˆæ¯” discovery/item æ›´ç¨³å®šï¼‰
        # ä¿ç•™åŸå§‹æŸ¥è¯¢å‚æ•°
        query_string = ""
        if "?" in url:
            query_string = "?" + url.split("?", 1)[1]
        explore_url = f"https://www.xiaohongshu.com/explore/{note_id}{query_string}"

        # å°è¯• explore é¡µé¢è§£æ
        try:
            return await self._parse_explore(explore_url, note_id)
        except XiaohongshuParseError:
            logger.debug("XHS explore parse failed, trying discovery")

        # å›é€€åˆ° discovery è§£æ
        if "/discovery/item/" in url:
            return await self._parse_discovery(url)

        raise XiaohongshuParseError("æ— æ³•è§£æå°çº¢ä¹¦å†…å®¹")

    async def _get_redirect_url(self, url: str) -> str:
        """è·å–çŸ­é“¾æ¥é‡å®šå‘ç›®æ ‡ï¼ˆå•æ¬¡é‡å®šå‘ï¼‰"""
        async with aiohttp.ClientSession(timeout=self.timeout, cookies=self.cookies) as session:
            async with session.get(url, headers=XHS_HEADERS, allow_redirects=False) as resp:
                if resp.status >= 400:
                    raise XiaohongshuParseError(f"çŸ­é“¾æ¥è¯·æ±‚å¤±è´¥: {resp.status}")
                location = resp.headers.get("Location", url)
                return location

    async def _parse_explore(self, url: str, note_id: str) -> XiaohongshuResult:
        """è§£æ explore é¡µé¢"""
        async with aiohttp.ClientSession(timeout=self.timeout, cookies=self.cookies) as session:
            async with session.get(url, headers=_EXPLORE_HEADERS) as resp:
                logger.debug("XHS explore url: %s, status: %s", resp.url, resp.status)
                if resp.status != 200:
                    raise XiaohongshuParseError(f"é¡µé¢è¯·æ±‚å¤±è´¥: {resp.status}")
                html = await resp.text()

        json_obj = self._extract_initial_state(html)

        # è·¯å¾„: ["note"]["noteDetailMap"][note_id]["note"]
        note_data = (
            json_obj.get("note", {})
            .get("noteDetailMap", {})
            .get(note_id, {})
            .get("note", {})
        )
        if not note_data:
            raise XiaohongshuParseError("æ— æ³•æ‰¾åˆ°ç¬”è®°è¯¦æƒ…æ•°æ®")

        return self._build_result_from_note(note_data, url)

    async def _parse_discovery(self, url: str) -> XiaohongshuResult:
        """è§£æ discovery é¡µé¢"""
        async with aiohttp.ClientSession(timeout=self.timeout, cookies=self.cookies) as session:
            async with session.get(url, headers=XHS_HEADERS, allow_redirects=True) as resp:
                logger.debug("XHS discovery url: %s, status: %s", resp.url, resp.status)
                if resp.status != 200:
                    raise XiaohongshuParseError(f"é¡µé¢è¯·æ±‚å¤±è´¥: {resp.status}")
                html = await resp.text()

        json_obj = self._extract_initial_state(html)

        note_data_wrapper = json_obj.get("noteData")
        if not note_data_wrapper:
            raise XiaohongshuParseError("æ— æ³•æ‰¾åˆ° noteData")

        preload_data = note_data_wrapper.get("normalNotePreloadData", {})
        note_data = note_data_wrapper.get("data", {}).get("noteData", {})
        if not note_data:
            raise XiaohongshuParseError("æ— æ³•æ‰¾åˆ° noteData.data.noteData")

        return self._build_result_from_note(note_data, url, preload_data)

    def _extract_initial_state(self, html: str) -> dict[str, Any]:
        """ä»HTMLä¸­æå– __INITIAL_STATE__ JSON"""
        pattern = r"window\.__INITIAL_STATE__=(.*?)</script>"
        match = re.search(pattern, html)
        if not match:
            raise XiaohongshuParseError("å°çº¢ä¹¦åˆ†äº«é“¾æ¥å¤±æ•ˆæˆ–å†…å®¹å·²åˆ é™¤")

        json_str = match.group(1).replace("undefined", "null")
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            raise XiaohongshuParseError(f"JSONè§£æå¤±è´¥: {e}") from e

    def _build_result_from_note(
        self,
        note: dict[str, Any],
        source_url: str,
        preload: dict[str, Any] | None = None,
    ) -> XiaohongshuResult:
        """ä»ç¬”è®°æ•°æ®æ„å»ºç»“æœ"""
        # æ ‡é¢˜å’Œæè¿°
        title = note.get("title") or note.get("shareTitle") or ""
        desc = note.get("desc") or note.get("shareDesc") or ""

        # ä½œè€…
        user = note.get("user") or note.get("author") or {}
        author = user.get("nickname") or user.get("nickName") or user.get("name")

        # å›¾ç‰‡åˆ—è¡¨ - è·å– URL å’Œ fileId
        image_list = note.get("imageList") or []
        image_urls = []
        file_ids = []
        for i, img in enumerate(image_list):
            if isinstance(img, dict):
                # è®°å½•è¯¦ç»†çš„å›¾ç‰‡æ•°æ®ä»¥ä¾¿æ’æŸ¥æ—§ç‰ˆç¬”è®°
                logger.debug(f"XHS Image[{i}] full data: {img}")
                
                # è·å–å›¾ç‰‡ URL
                img_url = self._get_original_image_url(img)
                if img_url:
                    image_urls.append(img_url)
                # è·å– fileId ç”¨äºå°è¯•åŸå›¾
                file_id = self._get_file_id_from_image(img)
                file_ids.append(file_id)  # å¯èƒ½æ˜¯ Noneï¼Œä¿æŒç´¢å¼•å¯¹åº”
        
        logger.debug(f"XHS Extracted file_ids: {file_ids}")

        # è§†é¢‘
        video_url = self._extract_video_url(note)

        # å°é¢ï¼ˆè§†é¢‘çš„ç¬¬ä¸€å¸§æˆ–ç¬¬ä¸€å¼ å›¾ç‰‡ï¼‰
        cover_url = None
        if preload:
            # preload ä¸­çš„å›¾ç‰‡é€šå¸¸æ˜¯æ— æ°´å°çš„
            preload_images = preload.get("imagesList") or []
            if preload_images and isinstance(preload_images[0], dict):
                cover_url = (
                    preload_images[0].get("urlSizeLarge")
                    or preload_images[0].get("url")
                )
        if not cover_url and image_urls:
            cover_url = image_urls[0]

        note_type = note.get("type", "")
        note_id = note.get("noteId") or note.get("id")

        logger.info(
            "XHS è§£æå®Œæˆ: type=%s, images=%d, video=%s",
            note_type, len(image_urls), "æœ‰" if video_url else "æ— "
        )

        return XiaohongshuResult(
            title=title if title else None,
            author=author,
            text=desc if desc else None,
            image_urls=image_urls,
            file_ids=file_ids,
            video_url=video_url,
            cover_url=cover_url,
            source_url=source_url,
            note_id=note_id,
        )

    def _get_original_image_url(self, img: dict[str, Any]) -> str | None:
        """ä»å›¾ç‰‡å¯¹è±¡è·å–æœ€ä½³å›¾ç‰‡ URL
        
        ä¼˜å…ˆçº§ï¼š
        1. urlDefault (é«˜æ¸…ï¼Œé€šå¸¸å¯ç”¨)
        2. url (æ™®é€š)
        å¦‚æœæœ‰ cookiesï¼Œä¼šåœ¨ handler å±‚å°è¯• ci.xiaohongshu.com åŸå›¾
        """
        # ä¼˜å…ˆä½¿ç”¨ urlDefaultï¼ˆé«˜æ¸…é¢„è§ˆï¼Œé€šå¸¸å¯ç”¨ï¼‰
        url = img.get("urlDefault") or img.get("url")
        if not url:
            return None
        
        # æ¸…ç†æ ·å¼åç¼€ä»¥è·å¾—æ›´é«˜æ¸…çš„å›¾ç‰‡
        if "!" in url:
            url = url.split("!", 1)[0]
        
        return url

    def _get_file_id_from_image(self, img: dict[str, Any]) -> str | None:
        """ä»å›¾ç‰‡å¯¹è±¡æå– fileIdï¼Œç”¨äºæ„å»ºåŸå›¾ URL"""
        # 1. å°è¯•ä»å­—æ®µè·å–
        file_id = img.get("fileId") or img.get("file_id") or img.get("traceId")
        
        # 2. å¦‚æœå­—æ®µæ²¡æœ‰ï¼Œä» urlDefault / url / urlPre ä¸­æå–
        if not file_id:
            for key in ("urlDefault", "url", "urlPre"):
                url = img.get(key)
                if isinstance(url, str) and url:
                    extracted = self._extract_file_id_from_url(url)
                    if extracted:
                        file_id = extracted
                        # logger.debug("XHS ä» URL %s æå–åˆ° fileId: %s", key, file_id)
                        break
        
        if file_id and "!" in file_id:
            file_id = file_id.split("!", 1)[0]
            
        return file_id

    @staticmethod
    def _extract_file_id_from_url(url: str) -> str | None:
        """ä» URL ä¸­æå– fileId"""
        if not url:
            return None
        # å¸¸è§æ ¼å¼ï¼š
        # 1. .../spectrum/1040g0k...
        # 2. .../notes_pre_post/1040g0k...
        # 3. .../1040g0k... (ç›´æ¥æ–‡ä»¶å)
        
        # åŒ¹é… spectrum åçš„ id
        if "spectrum/" in url:
            match = re.search(r"spectrum/([a-zA-Z0-9]+)", url)
            if match:
                return match.group(1)
                
        # åŒ¹é…ç›´æ¥çš„æ–‡ä»¶åï¼ˆé€šå¸¸æ˜¯20ä½ä»¥ä¸Šçš„å­—æ¯æ•°å­—ç»„åˆï¼‰
        # æ’é™¤ !style, ? ç­‰åç¼€
        base_name = url.split("?")[0].split("!")[0]
        base_name = base_name.split("/")[-1]
        
        if len(base_name) > 20 and re.match(r"^[a-zA-Z0-9]+$", base_name):
            return base_name
            
        return None

    def _extract_video_url(self, note: dict[str, Any]) -> str | None:
        """æå–è§†é¢‘URL"""
        if note.get("type") != "video":
            return None

        video = note.get("video")
        if not isinstance(video, dict):
            return None

        media = video.get("media")
        if not isinstance(media, dict):
            return None

        stream = media.get("stream")
        if not isinstance(stream, dict):
            return None

        # h265 æ— æ°´å°ä¼˜å…ˆï¼Œå…¶æ¬¡ h264
        for codec in ("h265", "h264", "av1", "h266"):
            codec_streams = stream.get(codec)
            if isinstance(codec_streams, list) and codec_streams:
                master_url = codec_streams[0].get("masterUrl")
                if master_url:
                    logger.debug("XHS video codec: %s", codec)
                    return master_url

        return None


# å¯¼å‡º
__all__ = [
    "XHS_HEADERS",
    "XHS_MESSAGE_PATTERN",
    "XHS_SHORT_LINK_PATTERN",
    "XiaohongshuExtractor",
    "XiaohongshuParseError",
    "XiaohongshuResult",
    "extract_xhs_links",
    "load_xhs_cookies",
]
