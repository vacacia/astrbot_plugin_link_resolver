# region å¯¼å…¥
import asyncio
import json
import re
import shutil
from dataclasses import dataclass
from http import cookiejar
from pathlib import Path
from urllib.parse import parse_qs, urlparse

import httpx
from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent
from astrbot.api.message_components import Image, Node, Nodes, Plain, Video
from bilibili_api import Credential, video
from bilibili_api.video import (
    AudioStreamDownloadURL,
    VideoCodecs,
    VideoDownloadURLDataDetecter,
    VideoQuality,
    VideoStreamDownloadURL,
)

from ..common import DOWNLOAD_HEADERS, BILIBILI_VIDEO_PATH, BILIBILI_THUMB_PATH, BILIBILI_CARD_PATH, BILI_COOKIES_FILE, SizeLimitExceeded
from ..common.card_renderer import UniversalCardRenderer, CardData, get_theme_for_platform
# endregion

# region å¸¸é‡ä¸æ­£åˆ™
BILI_VIDEO_URL_PATTERN = (
    r"(https?://)?(?:(?:www|m)\.)?bilibili\.com/video/(BV[0-9A-Za-z]{10}|av\d+)"
)
BILI_SHORT_LINK_PATTERN = r"https?://(?:b23\.tv|bili2233\.cn)/[A-Za-z\d._?%&+\-=/#]+"
BILI_BV_PATTERN = r"\bBV[0-9A-Za-z]{10}\b"
BILI_AV_PATTERN = r"\bav\d+\b"
BILI_MESSAGE_PATTERN = (
    rf"(?s).*(?:{BILI_VIDEO_URL_PATTERN}|{BILI_SHORT_LINK_PATTERN}|{BILI_BV_PATTERN}|{BILI_AV_PATTERN})"
)

QUALITY_ALIAS_MAP = {
    "åŸç”»": "ORIGINAL",
    "åŸç”»(æœ€é«˜ç”»è´¨)": "ORIGINAL",
    "æœ€é«˜": "ORIGINAL",
    "ORIGINAL": "ORIGINAL",
    "æœ€ä½": "LOWEST",
    "æœ€ä½ç”»è´¨": "LOWEST",
    "LOWEST": "LOWEST",
    "æœæ¯”è§†ç•Œ": "DOLBY",
    "æœæ¯”": "DOLBY",
    "DOLBY": "DOLBY",
    "HDR": "HDR",
    "8K": "8K",
    "4K": "4K",
    "1080P60": "1080P60",
    "1080Pé«˜å¸§ç‡": "1080P60",
    "1080P+": "1080PPLUS",
    "1080PPLUS": "1080PPLUS",
    "1080Pé«˜ç ç‡": "1080PPLUS",
    "1080P": "1080P",
    "720P60": "720P60",
    "720Pé«˜å¸§ç‡": "720P60",
    "720P": "720P",
    "480P": "480P",
    "360P": "360P",
    "240P": "240P",
}

CODECS_ALIAS_MAP = {
    "AVC": "AVC",
    "H264": "AVC",
    "H.264": "AVC",
    "HEVC": "HEVC",
    "H265": "HEVC",
    "H.265": "HEVC",
    "AV1": "AV1",
}
# endregion

# region è·¯å¾„å¸¸é‡ï¼ˆä½¿ç”¨ common å¯¼å‡ºçš„è·¯å¾„ï¼‰
BILI_VIDEO_PATH = BILIBILI_VIDEO_PATH
BILI_THUMBNAIL_PATH = BILIBILI_THUMB_PATH
BILI_QQ_THUMB_PATH = ""
# endregion

# region æ•°æ®ç±»
@dataclass
class VideoRef:
    bvid: str | None
    avid: int | None
    page_index: int
    source_url: str | None


@dataclass
class CookieStatus:
    is_login: bool
    is_vip: bool | None
    vip_type: int | None
    message: str


# endregion


# region Bç«™æ··å…¥
class BilibiliMixin:
    # region ç”»è´¨ä¸ç¼–ç 
    @staticmethod
    def _normalize_quality_alias(label: str) -> str:
        cleaned = label.strip()
        cleaned = cleaned.replace(" ", "")
        return QUALITY_ALIAS_MAP.get(cleaned, cleaned.upper())

    def _resolve_quality(self, alias: str) -> tuple[str, VideoQuality]:
        if alias == "ORIGINAL":
            return self._max_allowed_quality()
        if alias == "LOWEST":
            return self._min_allowed_quality()

        candidates = self._quality_name_candidates(alias)
        for name in candidates:
            if hasattr(VideoQuality, name):
                return name, getattr(VideoQuality, name)

        if alias in VideoQuality.__members__:
            return alias, VideoQuality[alias]

        fallback = "_720P"
        return fallback, getattr(VideoQuality, fallback)

    def _max_allowed_quality(self) -> tuple[str, VideoQuality]:
        candidates: list[VideoQuality] = []
        for quality in VideoQuality:
            name = quality.name.upper()
            if not self.allow_hdr and "HDR" in name:
                continue
            if not self.allow_dolby and "DOLBY" in name:
                continue
            candidates.append(quality)
        if not candidates:
            fallback = "_720P"
            return fallback, getattr(VideoQuality, fallback)
        best = max(candidates, key=lambda item: item.value)
        return best.name, best

    def _min_allowed_quality(self) -> tuple[str, VideoQuality]:
        candidates: list[VideoQuality] = []
        for quality in VideoQuality:
            name = quality.name.upper()
            if not self.allow_hdr and "HDR" in name:
                continue
            if not self.allow_dolby and "DOLBY" in name:
                continue
            candidates.append(quality)
        if not candidates:
            fallback = "_720P"
            return fallback, getattr(VideoQuality, fallback)
        lowest = min(candidates, key=lambda item: item.value)
        return lowest.name, lowest

    def _get_lower_qualities(self, current_quality: VideoQuality) -> list[VideoQuality]:
        candidates: list[VideoQuality] = []
        for quality in VideoQuality:
            name = quality.name.upper()
            if not self.allow_hdr and "HDR" in name:
                continue
            if not self.allow_dolby and "DOLBY" in name:
                continue
            if quality.value < current_quality.value:
                candidates.append(quality)
        return sorted(candidates, key=lambda q: q.value, reverse=True)

    @staticmethod
    def _quality_name_candidates(alias: str) -> list[str]:
        alias = alias.upper()
        candidates = [alias, f"_{alias}"]
        if "PLUS" in alias:
            candidates.append(alias.replace("PLUS", "_PLUS"))
            candidates.append(f"_{alias.replace('PLUS', '_PLUS')}")
        match = re.search(r"(\d+P)(\d+)", alias)
        if match:
            composite = f"{match.group(1)}_{match.group(2)}"
            candidates.extend([composite, f"_{composite}"])
        unique: list[str] = []
        seen: set[str] = set()
        for item in candidates:
            if item not in seen:
                seen.add(item)
                unique.append(item)
        return unique

    @staticmethod
    def _resolve_codecs(label: str) -> tuple[str, VideoCodecs]:
        normalized = label.strip().upper().replace(" ", "")
        normalized = CODECS_ALIAS_MAP.get(normalized, normalized)
        if hasattr(VideoCodecs, normalized):
            return normalized, getattr(VideoCodecs, normalized)
        return "AVC", VideoCodecs.AVC

    @staticmethod
    def _normalize_bvid(bvid: str) -> str | None:
        if not bvid:
            return None
        bvid = bvid.strip()
        if len(bvid) != 12:
            return None
        if bvid.startswith("BV"):
            candidate = bvid
        elif bvid[:2].lower() == "bv":
            candidate = "BV" + bvid[2:]
        else:
            return None
        return candidate if re.fullmatch(r"BV[0-9A-Za-z]{10}", candidate) else None
    # endregion

    # region é“¾æ¥ä¸è§£æ
    @staticmethod
    def _parse_page_index(text: str) -> int:
        try:
            parsed = urlparse(text)
            page = parse_qs(parsed.query).get("p", ["1"])[0]
            return max(int(page) - 1, 0)
        except Exception:
            return 0

    @staticmethod
    def _extract_bvid(text: str) -> str | None:
        match = re.search(BILI_BV_PATTERN, text)
        return match.group(0) if match else None

    @staticmethod
    def _extract_avid(text: str) -> int | None:
        match = re.search(BILI_AV_PATTERN, text, re.IGNORECASE)
        return int(match.group(0)[2:]) if match else None

    def extract_links_from_text(self, text: str, include_ids: bool = True) -> list[str]:
        links: list[str] = []
        url_patterns = [
            r"https?://(?:www\.)?bilibili\.com/video/[^\s\'\"<>]+",
            r"https?://m\.bilibili\.com/video/[^\s\'\"<>]+",
            r"https?://(?:b23\.tv|bili2233\.cn)/[^\s\'\"<>]+",
        ]
        for pattern in url_patterns:
            links.extend(re.findall(pattern, text, re.IGNORECASE))
        if include_ids:
            links.extend(re.findall(BILI_BV_PATTERN, text))
            links.extend(re.findall(BILI_AV_PATTERN, text, re.IGNORECASE))
        return links

    def _parse_video_ref_from_text(
        self, text: str, source_url: str | None = None
    ) -> VideoRef | None:
        if bvid := self._extract_bvid(text):
            bvid = self._normalize_bvid(bvid)
            if bvid:
                return VideoRef(
                    bvid=bvid,
                    avid=None,
                    page_index=self._parse_page_index(text),
                    source_url=source_url or text,
                )
        if avid := self._extract_avid(text):
            return VideoRef(
                bvid=None,
                avid=avid,
                page_index=self._parse_page_index(text),
                source_url=source_url or text,
            )
        return None

    async def resolve_short_url(self, short_url: str) -> str | None:
        try:
            async with httpx.AsyncClient(follow_redirects=True, timeout=10.0) as client:
                try:
                    response = await client.head(short_url)
                except Exception:
                    response = await client.get(short_url)
            final_url = str(response.url)
            logger.info("ğŸ”— çŸ­é“¾æ¥é‡å®šå‘: %s -> %s", short_url, final_url)
            return final_url
        except Exception as exc:
            logger.error("âŒ è§£æçŸ­é“¾æ¥å¤±è´¥: %s", str(exc))
        return None

    async def _resolve_video_ref_from_text(self, text: str) -> VideoRef | None:
        links = self.extract_links_from_text(text, include_ids=True)
        if not links:
            return None
        for token in links:
            if re.match(BILI_SHORT_LINK_PATTERN, token, re.IGNORECASE):
                final_url = await self.resolve_short_url(token)
                if final_url:
                    if ref := self._parse_video_ref_from_text(final_url, source_url=token):
                        return ref
                continue
            if ref := self._parse_video_ref_from_text(token):
                return ref
        return None

    async def _resolve_video_ref_from_links(self, links: list[str]) -> VideoRef | None:
        for link in links:
            if ref := await self._resolve_video_ref_from_text(link):
                return ref
        return None
    # endregion

    # region JSON å¡ç‰‡æå–
    def extract_bilibili_links_from_json(self, json_component) -> list[str]:
        links: list[str] = []
        try:
            json_data = self._coerce_json_payload(json_component)
            if not json_data:
                return links

            def search_json_for_links(obj):
                found: list[str] = []
                if isinstance(obj, dict):
                    for value in obj.values():
                        if isinstance(value, str):
                            found.extend(self.extract_links_from_text(value, include_ids=False))
                        elif isinstance(value, (dict, list)):
                            found.extend(search_json_for_links(value))
                elif isinstance(obj, list):
                    for item in obj:
                        if isinstance(item, str):
                            found.extend(self.extract_links_from_text(item, include_ids=False))
                        elif isinstance(item, (dict, list)):
                            found.extend(search_json_for_links(item))
                return found

            links.extend(search_json_for_links(json_data))

            if isinstance(json_data, dict):
                meta = json_data.get("meta", {})
                detail = meta.get("detail_1", {}) if meta else {}
                if detail:
                    for key in ("qqdocurl", "url"):
                        value = detail.get(key, "")
                        if value:
                            links.extend(self.extract_links_from_text(value, include_ids=False))

            logger.info("ä» JSON ç»„ä»¶ä¸­æå–åˆ°é“¾æ¥: %s", links)
        except Exception as exc:
            logger.warning("è§£æ JSON æ¶ˆæ¯ç»„ä»¶å¤±è´¥: %s", str(exc))
        return links
    # endregion

    # region Cookieå‡­è¯
    def _load_cookies(self) -> dict[str, str]:
        if not BILI_COOKIES_FILE.exists():
            return {}
        try:
            raw = BILI_COOKIES_FILE.read_text(encoding="utf-8").strip()
            if not raw:
                return {}
            if raw.lstrip().startswith("{"):
                data = json.loads(raw)
                if isinstance(data, dict):
                    return {str(k): str(v) for k, v in data.items()}
        except Exception:
            pass

        try:
            jar = cookiejar.MozillaCookieJar()
            jar.load(BILI_COOKIES_FILE, ignore_discard=True, ignore_expires=True)
            return {cookie.name: cookie.value for cookie in jar}
        except Exception as exc:
            logger.warning("ğŸª è¯»å– cookies å¤±è´¥: %s", str(exc))
            return {}

    def _build_credential(self, cookies: dict[str, str]) -> Credential:
        if not cookies:
            return Credential(sessdata=None)
        try:
            return Credential.from_cookies(cookies)
        except Exception as exc:
            logger.warning("ğŸª è¯»å– cookies å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–å‡­è¯: %s", str(exc))
            return Credential(sessdata=cookies.get("SESSDATA"))

    async def _check_cookie_status(self, cookies: dict[str, str]) -> CookieStatus:
        if not cookies:
            return CookieStatus(is_login=False, is_vip=None, vip_type=None, message="cookies ä¸ºç©º")
        try:
            async with httpx.AsyncClient(
                timeout=10.0,
                headers=DOWNLOAD_HEADERS,
                cookies=cookies,
                follow_redirects=True,
            ) as client:
                response = await client.get("https://api.bilibili.com/x/web-interface/nav")
            if response.status_code != 200:
                return CookieStatus(False, None, None, f"HTTP {response.status_code}")
            data = response.json()
            if data.get("code") != 0:
                return CookieStatus(False, None, None, f"code={data.get('code')}")
            info = data.get("data") or {}
            is_login = bool(info.get("isLogin"))
            vip = info.get("vip") or {}
            vip_status = vip.get("status") if isinstance(vip, dict) else None
            vip_type = vip.get("vipType") if isinstance(vip, dict) else None
            is_vip = vip_status == 1 if vip_status is not None else None
            message = "ok" if is_login else "not login"
            return CookieStatus(is_login, is_vip, vip_type, message)
        except Exception as exc:
            return CookieStatus(False, None, None, f"error: {exc}")
    # endregion

    # region Bç«™è§†é¢‘å¤„ç†
    async def _select_streams(
        self,
        video_obj: video.Video,
        page_index: int,
        video_quality: VideoQuality | None = None,
    ) -> tuple:
        """é€‰æ‹©è§†é¢‘æµå’ŒéŸ³é¢‘æµã€‚å¯æŒ‡å®šç”»è´¨ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®çš„é»˜è®¤ç”»è´¨ã€‚
        
        Returns:
            tuple: (video_stream, audio_stream, estimated_size_mb)
                - estimated_size_mb: ä» API çš„ bandwidth å’Œ timelength è®¡ç®—çš„é¢„ä¼°å¤§å° (MB)ï¼Œå¦‚æœæ— æ³•è®¡ç®—åˆ™ä¸º None
        """
        target_quality = video_quality or self.video_quality
        download_url_data = await video_obj.get_download_url(page_index=page_index)
        detecter = VideoDownloadURLDataDetecter(download_url_data)
        streams = detecter.detect_best_streams(
            video_max_quality=target_quality,
            codecs=[self.video_codecs],
            no_dolby_video=not self.allow_dolby,
            no_hdr=not self.allow_hdr,
        )
        if not streams:
            raise RuntimeError("æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘æµ")
        video_stream = streams[0]
        if not isinstance(video_stream, VideoStreamDownloadURL):
            raise RuntimeError("æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘æµ")

        audio_stream = None
        if len(streams) > 1 and isinstance(streams[1], AudioStreamDownloadURL):
            audio_stream = streams[1]

        # ä»åŸå§‹ API æ•°æ®è®¡ç®—é¢„ä¼°æ–‡ä»¶å¤§å°
        estimated_size_mb = self._estimate_size_from_api_data(
            download_url_data, video_stream, audio_stream
        )

        logger.info(
            "ğŸï¸ å®é™…é€‰ç”¨ç”»è´¨: %s, ç¼–ç : %s, é¢„ä¼°å¤§å°: %s MB",
            video_stream.video_quality.name,
            video_stream.video_codecs,
            f"{estimated_size_mb:.2f}" if estimated_size_mb else "æœªçŸ¥",
        )
        return video_stream, audio_stream, estimated_size_mb

    def _estimate_size_from_api_data(
        self,
        download_url_data: dict,
        video_stream: VideoStreamDownloadURL,
        audio_stream: AudioStreamDownloadURL | None,
    ) -> float | None:
        """ä» API è¿”å›çš„ bandwidth å’Œ timelength å­—æ®µè®¡ç®—é¢„ä¼°æ–‡ä»¶å¤§å°ã€‚
        
        å…¬å¼: size_bytes = bandwidth * (timelength / 1000) / 8
        """
        try:
            dash = download_url_data.get("dash")
            if not dash:
                return None
            
            timelength_ms = download_url_data.get("timelength")  # æ¯«ç§’
            if not timelength_ms:
                return None
            timelength_sec = timelength_ms / 1000
            
            total_bandwidth = 0
            
            # æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘æµ bandwidth
            video_url = video_stream.url
            for v in dash.get("video", []):
                v_url = v.get("baseUrl") or v.get("base_url", "")
                if v_url == video_url:
                    total_bandwidth += v.get("bandwidth", 0)
                    break
            
            # æŸ¥æ‰¾åŒ¹é…çš„éŸ³é¢‘æµ bandwidth
            if audio_stream:
                audio_url = audio_stream.url
                for a in dash.get("audio", []):
                    a_url = a.get("baseUrl") or a.get("base_url", "")
                    if a_url == audio_url:
                        total_bandwidth += a.get("bandwidth", 0)
                        break
            
            if total_bandwidth == 0:
                return None
            
            # bandwidth å•ä½æ˜¯ bps (bits per second)
            size_bytes = total_bandwidth * timelength_sec / 8
            size_mb = size_bytes / 1024 / 1024
            return size_mb
        except Exception as exc:
            logger.debug("ä» API æ•°æ®è®¡ç®—æ–‡ä»¶å¤§å°å¤±è´¥: %s", str(exc))
            return None

    async def _download_video(
        self,
        video_obj: video.Video,
        bvid: str,
        page_index: int,
        page_count: int,
        cookies: dict[str, str],
    ) -> tuple[Path, str]:
        """ä¸‹è½½è§†é¢‘ã€‚å¦‚æœè¶…è¿‡å¤§å°é™åˆ¶ä¸”å¼€å¯äº†è‡ªåŠ¨é™ç”»è´¨ï¼Œä¼šå°è¯•æ›´ä½ç”»è´¨ã€‚"""
        current_quality = self.video_quality
        max_bytes = self.max_video_size_mb * 1024 * 1024 if self.max_video_size_mb > 0 else None

        while True:
            video_stream, audio_stream, size_mb = await self._select_streams(
                video_obj, page_index, video_quality=current_quality
            )
            video_url = video_stream.url
            audio_url = audio_stream.url if audio_stream else None
            actual_quality = video_stream.video_quality

            # size_mb ç°åœ¨ä» API çš„ bandwidth å’Œ timelength è®¡ç®—ï¼Œæ¯” HTTP HEAD æ›´å¯é 
            if size_mb is None and max_bytes is not None:
                logger.warning(
                    "âš ï¸ æ— æ³•ä» API è·å–è§†é¢‘å¤§å°ä¼°ç®—ï¼Œé™ç”»è´¨åŠŸèƒ½å¯èƒ½ä¸ç”Ÿæ•ˆ"
                )

            size_exceeds = (
                size_mb is not None
                and max_bytes is not None
                and size_mb > self.max_video_size_mb
            )

            if size_exceeds:
                if self.allow_quality_fallback:
                    lower_qualities = self._get_lower_qualities(actual_quality)
                    if lower_qualities:
                        next_quality = lower_qualities[0]
                        logger.info(
                            "âš ï¸ ç”»è´¨ %s è¶…é™ (%.2fMB > %dMB)ï¼Œå°è¯•é™è‡³ %s",
                            actual_quality.name,
                            size_mb,
                            self.max_video_size_mb,
                            next_quality.name,
                        )
                        current_quality = next_quality
                        continue
                # æ— æ³•é™çº§æˆ–ç¦ç”¨é™çº§
                raise SizeLimitExceeded("è¶…è¿‡å¤§å°é™åˆ¶")

            # å¤§å°åˆé€‚ï¼Œå¼€å§‹ä¸‹è½½
            break

        suffix = f"_p{page_index + 1}" if page_count > 1 else ""
        output_path = BILI_VIDEO_PATH / f"{bvid}{suffix}.mp4"

        if audio_url:
            temp_video = output_path.with_suffix(".video")
            temp_audio = output_path.with_suffix(".audio")
            await self._download_stream(video_url, temp_video, cookies, max_bytes)
            await self._download_stream(audio_url, temp_audio, cookies, max_bytes)
            await self._merge_av(temp_video, temp_audio, output_path)
        else:
            await self._download_stream(video_url, output_path, cookies, max_bytes)

        return output_path, actual_quality.name

    async def _get_video_info(self, video_obj: video.Video, source_tag: str = "") -> dict:
        """è·å–è§†é¢‘ä¿¡æ¯ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        retry_count = getattr(self, 'retry_count', 3)
        last_error: Exception | None = None
        
        for attempt in range(retry_count + 1):
            try:
                return await video_obj.get_info()
            except asyncio.CancelledError:
                raise
            except asyncio.TimeoutError as exc:
                last_error = exc
                if attempt < retry_count:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿: 1s, 2s, 4s...
                    logger.warning(
                        "âš ï¸ Bç«™è§†é¢‘ä¿¡æ¯è·å–è¶…æ—¶%s, %dç§’åé‡è¯• (%d/%d)",
                        source_tag, wait_time, attempt + 1, retry_count
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error("âŒ Bç«™è§†é¢‘ä¿¡æ¯è·å–è¶…æ—¶%s (å·²é‡è¯•%dæ¬¡)", source_tag, retry_count)
            except Exception as exc:
                last_error = exc
                # æ£€æŸ¥æ˜¯å¦ä¸º curl è¶…æ—¶é”™è¯¯
                error_str = str(exc).lower()
                is_timeout = "timeout" in error_str or "curl: (28)" in error_str
                
                if is_timeout and attempt < retry_count:
                    wait_time = 2 ** attempt
                    logger.warning(
                        "âš ï¸ Bç«™è§†é¢‘ä¿¡æ¯è·å–å¤±è´¥%s: %s, %dç§’åé‡è¯• (%d/%d)",
                        source_tag, str(exc), wait_time, attempt + 1, retry_count
                    )
                    await asyncio.sleep(wait_time)
                elif attempt < retry_count and self._is_retryable_error(exc):
                    wait_time = 2 ** attempt
                    logger.warning(
                        "âš ï¸ Bç«™è§†é¢‘ä¿¡æ¯è·å–å¤±è´¥%s: %s, %dç§’åé‡è¯• (%d/%d)",
                        source_tag, str(exc), wait_time, attempt + 1, retry_count
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error("âŒ Bç«™è§†é¢‘ä¿¡æ¯è·å–å¤±è´¥%s: %s (å·²é‡è¯•%dæ¬¡)", source_tag, str(exc), retry_count)
                    break
        
        if last_error:
            raise last_error
        raise RuntimeError("è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥")
    
    @staticmethod
    def _is_retryable_error(exc: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„ç½‘ç»œé”™è¯¯"""
        error_str = str(exc).lower()
        retryable_patterns = [
            "timeout", "timed out", "connection", "reset", "refused",
            "curl:", "network", "temporary", "unavailable", "503", "502"
        ]
        return any(pattern in error_str for pattern in retryable_patterns)

    async def _download_bili_cover(self, cover_url: str, bvid: str) -> Path | None:
        """ä¸‹è½½å°é¢å›¾åˆ°ç¼“å­˜ç›®å½•"""
        if not cover_url:
            return None
        try:
            cover_path = BILIBILI_CARD_PATH / f"{bvid}_cover.jpg"
            await self._download_stream(cover_url, cover_path, cookies=None, max_bytes=None)
            return cover_path
        except Exception as exc:
            logger.warning("âŒ ä¸‹è½½Bç«™å°é¢å¤±è´¥: %s", str(exc))
            return None

    def _format_count(self, count: int) -> str:
        """æ ¼å¼åŒ–æ•°å­—ä¸ºæ˜“è¯»å½¢å¼"""
        if count >= 100000000:
            return f"{count / 100000000:.1f}äº¿"
        if count >= 10000:
            return f"{count / 10000:.1f}ä¸‡"
        return str(count)

    async def _render_bili_card(
        self,
        *,
        title: str,
        author: str,
        cover_url: str,
        bvid: str,
        views: int,
        likes: int,
        coins: int,
    ) -> Path | None:
        """æ¸²æŸ“Bç«™è§†é¢‘å¡ç‰‡"""
        try:
            cover_path = await self._download_bili_cover(cover_url, bvid)

            theme = get_theme_for_platform("bilibili")
            renderer = UniversalCardRenderer(theme)

            data = CardData(
                title=title,
                author=author,
                text=None,
                image_paths=[],
                cover_path=cover_path,
                is_video=True,
                stats={
                    "ğŸ‘": self._format_count(views),
                    "ğŸ‘": self._format_count(likes),
                    "ğŸª™": self._format_count(coins),
                },
            )

            card_img = renderer.render(data)
            card_path = BILIBILI_CARD_PATH / f"{bvid}_card.png"
            card_img.save(card_path)

            logger.info("âœ… Bç«™å¡ç‰‡æ¸²æŸ“æˆåŠŸ: %s", card_path)
            return card_path
        except Exception as exc:
            logger.warning("âŒ Bç«™å¡ç‰‡æ¸²æŸ“å¤±è´¥: %s", str(exc))
            return None
    # endregion

    # region Bç«™ä¸»å¤„ç†
    async def _process_bili_video(
        self, event: AstrMessageEvent, ref: VideoRef, is_from_card: bool = False
    ):
        import time as time_module
        process_start = time_module.perf_counter()
        timing = {}  # è®°å½•å„æ­¥éª¤è€—æ—¶
        
        self._refresh_config()
        if not self.bili_enabled:
            return
            
        dedup_key = ref.bvid or str(ref.avid or "")
        if dedup_key and not self._check_and_record_url(f"bili:{dedup_key}"):
            return
            
        source_tag = "(æ¥è‡ªå¡ç‰‡)" if is_from_card else ""
        await self._send_reaction_emoji(event, source_tag)

        cookies = self._load_cookies()
        credential = self._build_credential(cookies)
        cookie_status = await self._check_cookie_status(cookies)
        logger.debug(
            "ğŸª Cookieæ£€æµ‹%s: ç™»å½•=%s, ä¼šå‘˜=%s, vipType=%s, çŠ¶æ€=%s",
            source_tag,
            cookie_status.is_login,
            cookie_status.is_vip,
            cookie_status.vip_type,
            cookie_status.message,
        )

        if ref.bvid:
            video_obj = video.Video(bvid=ref.bvid, credential=credential)
        elif ref.avid:
            video_obj = video.Video(aid=ref.avid, credential=credential)
        else:
            return

        try:
            # region è§£æé˜¶æ®µ
            parse_start = time_module.perf_counter()
            try:
                info = await self._get_video_info(video_obj, source_tag)
            except asyncio.CancelledError:
                raise
            except Exception:
                # é”™è¯¯æ—¥å¿—å·²åœ¨ _get_video_info ä¸­è¾“å‡º
                return
            timing["parse"] = time_module.perf_counter() - parse_start
            # endregion

            stat = info.get("stat", {})
            bvid = info.get("bvid") or ref.bvid
            if not bvid:
                logger.warning("âŒ æ— æ³•è·å– bvid%s", source_tag)
                return

            title = info.get("title", "æœªçŸ¥æ ‡é¢˜")
            up_name = info.get("owner", {}).get("name", "æœªçŸ¥UPä¸»")
            duration_seconds = info.get("duration", 0)
            view_count = stat.get("view", 0)
            likes = stat.get("like", 0)
            coins = stat.get("coin", 0)
            shares = stat.get("share", 0)
            comments = stat.get("reply", 0)
            cover_url = info.get("pic", "")

            logger.debug(
                "ï¿½ Bç«™è§£æå®Œæˆ%s: bvid=%s, æ ‡é¢˜=%s, è§£æè€—æ—¶=%.2fs",
                source_tag,
                bvid,
                title[:30],
                timing["parse"],
            )

            pages = info.get("pages") or []
            page_count = len(pages) if pages else 1
            page_index = min(ref.page_index, max(page_count - 1, 0))
            has_page_param = bool(ref.source_url and re.search(r"[?&]p=\\d+", ref.source_url))
            is_multi_page = page_count > 1 and self.enable_multi_page and not has_page_param
            page_indexes = [page_index]
            if is_multi_page:
                page_indexes = list(range(min(self.multi_page_max, page_count)))

            video_paths: list[Path] = []
            thumbnail_paths: list[Path] = []

            # region ä¸‹è½½é˜¶æ®µ
            download_start = time_module.perf_counter()
            
            if is_multi_page:
                nodes = Nodes([])
                self_id = str(event.get_self_id())
                header_text = (
                    f"ğŸ¬ æ ‡é¢˜: {title}\n"
                    f"ğŸ‘¤ UPä¸»: {up_name}\n"
                    f"ğŸ“„ åˆ†Pæ•°é‡: {page_count}\n"
                    f"ğŸ”¢ æ’­æ”¾é‡: {view_count}\n"
                    f"â¤ï¸ ç‚¹èµ: {likes}\n"
                    f"ğŸ† æŠ•å¸: {coins}\n"
                    f"ğŸ”„ åˆ†äº«: {shares}\n"
                    f"ğŸ’¬ è¯„è®º: {comments}\n"
                    f"ğŸšï¸ ç”»è´¨è®¾ç½®: {self.quality_label}"
                )
                nodes.nodes.append(Node(uin=self_id, name="BiliBot", content=[Plain(header_text)]))

                for idx in page_indexes:
                    page_info = pages[idx] if idx < len(pages) else {}
                    page_title = page_info.get("part") or title
                    page_duration = page_info.get("duration") or duration_seconds
                    try:
                        page_start = time_module.perf_counter()
                        video_path, actual_quality = await self._download_video(
                            video_obj,
                            bvid,
                            idx,
                            page_count,
                            cookies,
                        )
                        video_paths.append(video_path)
                        page_elapsed = time_module.perf_counter() - page_start
                        logger.debug(
                            "ï¿½ Bç«™åˆ†Pä¸‹è½½æˆåŠŸ%s [%d/%d]: size=%.2fMB, ç”»è´¨=%s, è€—æ—¶=%.2fs",
                            source_tag, idx + 1, len(page_indexes),
                            video_path.stat().st_size / 1024 / 1024,
                            actual_quality,
                            page_elapsed
                        )
                        page_text = (
                            f"ğŸ“„ åˆ†P {idx + 1}/{page_count}: {page_title}\n"
                            f"â±ï¸ æ—¶é•¿: {page_duration // 60}:{page_duration % 60:02d}\n"
                            f"ğŸï¸ å®é™…ç”»è´¨: {actual_quality}"
                        )
                        nodes.nodes.append(Node(uin=self_id, name="BiliBot", content=[Plain(page_text)]))
                        abs_video_path = str(video_path.resolve())
                        nodes.nodes.append(
                            Node(uin=self_id, name="BiliBot", content=[Video.fromFileSystem(abs_video_path)])
                        )
                    except asyncio.CancelledError:
                        raise
                    except SizeLimitExceeded:
                        warn_text = (
                            f"âŒ åˆ†P {idx + 1} è¶…è¿‡å¤§å°é™åˆ¶ ({self.max_video_size_mb}MB)ï¼Œè·³è¿‡ä¸‹è½½"
                        )
                        nodes.nodes.append(Node(uin=self_id, name="BiliBot", content=[Plain(warn_text)]))
                    except Exception as exc:
                        logger.error("âŒ è§†é¢‘ä¸‹è½½å¤±è´¥%s: %s", source_tag, str(exc))
                        error_text = f"âŒ åˆ†P {idx + 1} ä¸‹è½½å¤±è´¥: {str(exc)}"
                        nodes.nodes.append(Node(uin=self_id, name="BiliBot", content=[Plain(error_text)]))

                timing["download"] = time_module.perf_counter() - download_start
                
                # region å‘é€é˜¶æ®µ
                send_start = time_module.perf_counter()
                yield event.chain_result([nodes])
                timing["send"] = time_module.perf_counter() - send_start
                # endregion

                if BILI_QQ_THUMB_PATH and cover_url and video_paths:
                    for path in video_paths:
                        video_md5 = self.calculate_md5(path)
                        thumbnail_save_path = BILI_THUMBNAIL_PATH / f"{video_md5}.png"
                        qq_thumb_path = Path(BILI_QQ_THUMB_PATH) / f"{video_md5}_0.png"
                        if await self.download_thumbnail(cover_url, thumbnail_save_path):
                            shutil.copy(thumbnail_save_path, qq_thumb_path)
                            thumbnail_paths.append(thumbnail_save_path)

                # è¾“å‡ºå®Œæ•´è€—æ—¶æ—¥å¿—
                total_elapsed = time_module.perf_counter() - process_start
                logger.info(
                    "ğŸ¬ Bç«™å¤„ç†å®Œæˆ%s: æ ‡é¢˜=%s, åˆ†P=%d | è€—æ—¶: è§£æ=%.2fs, ä¸‹è½½=%.2fs, å‘é€=%.2fs, æ€»è®¡=%.2fs",
                    source_tag,
                    title[:20],
                    len(video_paths),
                    timing.get("parse", 0),
                    timing.get("download", 0),
                    timing.get("send", 0),
                    total_elapsed,
                )

                if video_paths or thumbnail_paths:
                    asyncio.create_task(self.cleanup_files(video_paths, thumbnail_paths))
                return

            # å•Pè§†é¢‘å¤„ç†
            try:
                video_path, actual_quality = await self._download_video(
                    video_obj, bvid, page_index, page_count, cookies
                )
                video_paths.append(video_path)
                logger.debug(
                    "ğŸ“¥ Bç«™è§†é¢‘ä¸‹è½½æˆåŠŸ%s: size=%.2fMB, ç”»è´¨=%s, è€—æ—¶=%.2fs",
                    source_tag,
                    video_path.stat().st_size / 1024 / 1024,
                    actual_quality,
                    time_module.perf_counter() - download_start
                )
            except asyncio.CancelledError:
                raise
            except SizeLimitExceeded:
                video_info_text = (
                    f"ğŸ¬ æ ‡é¢˜: {title}\n"
                    f"ğŸ‘¤ UPä¸»: {up_name}\n"
                    f"ğŸ”¢ æ’­æ”¾é‡: {view_count}\n"
                    f"â¤ï¸ ç‚¹èµ: {likes}\n"
                    f"ğŸ† æŠ•å¸: {coins}\n"
                    f"ğŸ”„ åˆ†äº«: {shares}\n"
                    f"ğŸ’¬ è¯„è®º: {comments}\n\n"
                    f"âŒ è§†é¢‘å¤§å°è¶…è¿‡é™åˆ¶ ({self.max_video_size_mb}MB)ï¼Œæ— æ³•ä¸‹è½½\n"
                    f"ğŸ’¡ å½“å‰ç”»è´¨è®¾ç½®: {self.quality_label}"
                )
                yield event.plain_result(video_info_text)
                return
            except Exception as exc:
                logger.error("âŒ è§†é¢‘ä¸‹è½½å¤±è´¥%s: %s", source_tag, str(exc))
                yield event.plain_result(f"âŒ è§†é¢‘ä¸‹è½½å¤±è´¥: {str(exc)}")
                return

            timing["download"] = time_module.perf_counter() - download_start
            # endregion

            # region æ¸²æŸ“é˜¶æ®µ
            render_start = time_module.perf_counter()
            card_path = None
            if self.bili_merge_send:
                card_path = await self._render_bili_card(
                    title=title,
                    author=up_name,
                    cover_url=cover_url,
                    bvid=bvid,
                    views=view_count,
                    likes=likes,
                    coins=coins,
                )
            timing["render"] = time_module.perf_counter() - render_start
            # endregion

            # region å‘é€é˜¶æ®µ
            send_start = time_module.perf_counter()
            
            try:
                abs_video_path = str(video_path.resolve())
                video_component = Video.fromFileSystem(abs_video_path)
                
                if self.bili_merge_send:
                    nodes = Nodes([])
                    self_id = str(event.get_self_id())
                    
                    if card_path and card_path.exists():
                        card_component = Image.fromFileSystem(str(card_path.resolve()))
                        nodes.nodes.append(
                            Node(uin=self_id, name="BiliBot", content=[card_component])
                        )

                    nodes.nodes.append(
                        Node(uin=self_id, name="BiliBot", content=[video_component])
                    )
                    logger.debug("ğŸš€ Bç«™åˆå¹¶æ¶ˆæ¯å‡†å¤‡å‘é€%s: èŠ‚ç‚¹æ•°=%d", source_tag, len(nodes.nodes))
                    yield event.chain_result([nodes])
                else:
                    # éåˆå¹¶è½¬å‘ï¼šåªå‘è§†é¢‘
                    logger.debug("ï¿½ Bç«™æ™®é€šæ¶ˆæ¯å‡†å¤‡å‘é€%s", source_tag)
                    yield event.chain_result([video_component])

                timing["send"] = time_module.perf_counter() - send_start
                # endregion

                if BILI_QQ_THUMB_PATH and cover_url:
                    video_md5 = self.calculate_md5(video_path)
                    thumbnail_save_path = BILI_THUMBNAIL_PATH / f"{video_md5}.png"
                    qq_thumb_path = Path(BILI_QQ_THUMB_PATH) / f"{video_md5}_0.png"
                    if await self.download_thumbnail(cover_url, thumbnail_save_path):
                        shutil.copy(thumbnail_save_path, qq_thumb_path)
                        thumbnail_paths.append(thumbnail_save_path)

                # è¾“å‡ºå®Œæ•´è€—æ—¶æ—¥å¿—
                total_elapsed = time_module.perf_counter() - process_start
                logger.info(
                    "ğŸ¬ Bç«™å¤„ç†å®Œæˆ%s: æ ‡é¢˜=%s, ç”»è´¨=%s | è€—æ—¶: è§£æ=%.2fs, ä¸‹è½½=%.2fs, æ¸²æŸ“=%.2fs, å‘é€=%.2fs, æ€»è®¡=%.2fs",
                    source_tag,
                    title[:20],
                    actual_quality,
                    timing.get("parse", 0),
                    timing.get("download", 0),
                    timing.get("render", 0),
                    timing.get("send", 0),
                    total_elapsed,
                )

                if video_paths or thumbnail_paths:
                    asyncio.create_task(self.cleanup_files(video_paths, thumbnail_paths))
            except asyncio.CancelledError:
                raise
            except Exception as exc:
                logger.error("âŒ è§†é¢‘å‘é€å¤±è´¥%s: %s", source_tag, str(exc))
                yield event.plain_result(f"âŒ è§†é¢‘å‘é€å¤±è´¥: {str(exc)}")
                if video_paths or thumbnail_paths:
                    asyncio.create_task(self.cleanup_files(video_paths, thumbnail_paths))
        except asyncio.CancelledError:
            logger.info("â™»ï¸ Bç«™è§£æä»»åŠ¡å·²ä¸­æ–­%s", source_tag)
            return
    # endregion

    # region äº‹ä»¶å¤„ç†å™¨
    # äº‹ä»¶è¿‡æ»¤å™¨ç”± main.py æ³¨å†Œï¼Œç¡®ä¿ç»‘å®šæ’ä»¶å®ä¾‹ã€‚
    async def handle_bili_video(self, event: AstrMessageEvent):
        if not self.bili_enabled:
            return
        if self._is_self_message(event):
            return
        if await self._is_bot_muted(event):
            return
        self._register_parse_task("bili", event)
        event.should_call_llm(True)
        try:
            ref = await self._resolve_video_ref_from_text(event.message_str)
            if not ref:
                return
            async for result in self._process_bili_video(event, ref=ref, is_from_card=False):
                yield result
        except asyncio.CancelledError:
            logger.info("â™»ï¸ Bç«™è§£æä»»åŠ¡å·²ä¸­æ–­")
            return
    # endregion


# endregion
